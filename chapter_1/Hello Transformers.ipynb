{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2b18b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f3b3a53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abdelrahman/.local/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-02-28 13:01:52.946226: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-28 13:01:53.217216: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-02-28 13:01:53.217237: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-02-28 13:01:54.120204: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-28 13:01:54.120317: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-28 13:01:54.120330: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce983ba6",
   "metadata": {},
   "source": [
    "## A Tour of Transformer Applications\n",
    "\n",
    "We will see some progress bars as hugging face will start to install the model weights into our local machine from the Hugging Face Hub.\n",
    "The second time you instantiate the pipeline, the library will notice that youâ€™ve already downloaded the weights and will use the cached version instead.\n",
    "\n",
    "If no frame work is specified the hugging face will install the original framework used with the model, other wise you can use the framework parameter to specify the framework you need to be used, and this is good because it maybe install the two versions of the model with pytorch or tensorflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10c0b2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"I have paid out the Immigration Health Surcharge with this reference number \"IHS109092150PA01\", and the IHS sent me an email 'The transaction reference for your payment of USD 3,950.41 is IHS109092150PA01|2023-01-26T11:05:00. On completion of your IHS application you will also be emailed a copy of your IHS reference number, which you will need for your immigration application.'\n",
    "\n",
    "I have attached a screenshot of the payment.\n",
    "\n",
    "\n",
    "But the Immigration health surcharge sends me on Feb 7, 2023, top-up payment is needed and the email format is quoted below:\n",
    "\n",
    "'Name: MR Abdelrahman Hamdy Rezk\n",
    "\n",
    "IHS reference number: IHS017333073\n",
    "\n",
    "A change has been made to the details you provided. You need to pay an additional payment of 3964.2 (USD) by 14 February 2023.'\n",
    "\n",
    "I have attached a screenshot of the email.\n",
    "\n",
    "\n",
    "So, you have asked me to pay on this reference number: IHS017333073 which has also been on my application, but I have paid out and you have sent me an email with the payment with this reference number: IHS109092150. So please try to solve this problem because they asked me to pay by 14 February 2023, and I already paid, and you have directed me to the next step of my application with another reference number, and I have no option to add the one you have sent me after the IHS payment, so you have to change my payment number of IHS to the IHS number mentioned in my application and you have asked me to pay for it.\n",
    "\n",
    "I have attached all screenshots required.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5165c0",
   "metadata": {},
   "source": [
    "# Text Classification\n",
    "\n",
    "Most of the NLP applications start to know your customer opinion about your services or anything, know the people's sentiments about anything is very helpful, and it takes no time to know the sentiment of some text.\n",
    "This can be done in just few lines of code using Huggingface transformers, but we will go deeper o fine-tune these models in chapter 2.\n",
    "\n",
    "Huggingface Transformers allow you to use its apis at various level of abstraction as you need:\n",
    "- Direct use of the model (As we use the pipeline for the task directly)\n",
    "- Build your own model\n",
    "- load the model in your perfared framework\n",
    "- Fine tune the model\n",
    "\n",
    "The pipelines is abstract away all the steps needed to convert raw text into a set of predictions from a fine-tuned model.\n",
    "\n",
    "pipelines is used by provide the nlp task you need to done on the provided text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bdc45e0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>0.997495</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      label     score\n",
       "0  NEGATIVE  0.997495"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = pipeline(\"text-classification\")\n",
    "# We can use list of texts, and it will also return a list of predictions, and use DataFrame to be more visible\n",
    "outputs = classifier(text)\n",
    "pd.DataFrame(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e15b14",
   "metadata": {},
   "source": [
    "## Named Entity Recognition\n",
    "\n",
    "Now we have known that its a negative comment from one of our customer, and we need to know what is the complains is about, we need to know much more information instead of reading this long complains, we need to know this compain is about a particular item or service, this known as NER, which extract the objects and classify into some of the categories like its about place or product or services and so on.\n",
    "\n",
    "We can see how the model extracts the objects from the provided text, as well as give it an entity, and aggregation_strategy simple is to use the categories defined by the model.\n",
    "\n",
    "The scores tell us how confident the model was about the entities it identified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08050ba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision f2482bf (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>entity_group</th>\n",
       "      <th>score</th>\n",
       "      <th>word</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ORG</td>\n",
       "      <td>0.846629</td>\n",
       "      <td>I</td>\n",
       "      <td>77</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ORG</td>\n",
       "      <td>0.996680</td>\n",
       "      <td>IHS</td>\n",
       "      <td>104</td>\n",
       "      <td>107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ORG</td>\n",
       "      <td>0.974536</td>\n",
       "      <td>IHS</td>\n",
       "      <td>248</td>\n",
       "      <td>251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ORG</td>\n",
       "      <td>0.978925</td>\n",
       "      <td>IHS</td>\n",
       "      <td>304</td>\n",
       "      <td>307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ORG</td>\n",
       "      <td>0.959406</td>\n",
       "      <td>Immigration</td>\n",
       "      <td>437</td>\n",
       "      <td>448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>PER</td>\n",
       "      <td>0.890806</td>\n",
       "      <td>Abdelrahman Hamdy Rezk</td>\n",
       "      <td>565</td>\n",
       "      <td>587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ORG</td>\n",
       "      <td>0.936408</td>\n",
       "      <td>IHS</td>\n",
       "      <td>589</td>\n",
       "      <td>592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ORG</td>\n",
       "      <td>0.916784</td>\n",
       "      <td>IHS</td>\n",
       "      <td>1275</td>\n",
       "      <td>1278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ORG</td>\n",
       "      <td>0.995584</td>\n",
       "      <td>IHS</td>\n",
       "      <td>1331</td>\n",
       "      <td>1334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ORG</td>\n",
       "      <td>0.943982</td>\n",
       "      <td>IHS</td>\n",
       "      <td>1342</td>\n",
       "      <td>1345</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  entity_group     score                    word  start   end\n",
       "0          ORG  0.846629                       I     77    78\n",
       "1          ORG  0.996680                     IHS    104   107\n",
       "2          ORG  0.974536                     IHS    248   251\n",
       "3          ORG  0.978925                     IHS    304   307\n",
       "4          ORG  0.959406             Immigration    437   448\n",
       "5          PER  0.890806  Abdelrahman Hamdy Rezk    565   587\n",
       "6          ORG  0.936408                     IHS    589   592\n",
       "7          ORG  0.916784                     IHS   1275  1278\n",
       "8          ORG  0.995584                     IHS   1331  1334\n",
       "9          ORG  0.943982                     IHS   1342  1345"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Same we use pipeline but we have point to another nlp task which is ner\n",
    "ner_tagger = pipeline('ner', aggregation_strategy=\"simple\")\n",
    "outputs = ner_tagger(text)\n",
    "pd.DataFrame(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6628a05",
   "metadata": {},
   "source": [
    "## Question Answering\n",
    "Extracting all the named entities in a text is nice, but sometimes we would like to ask more targeted questions. This is where we can use question answering.\n",
    "\n",
    "We provide the model with the context(our text) along with questions we need to get answers for this text.\n",
    "\n",
    "Some questions and answering systems have extracted the answer from the text itself and its called extractive question answering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "835597f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-cased-distilled-squad and revision 626af31 (https://huggingface.co/distilbert-base-cased-distilled-squad).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00029</td>\n",
       "      <td>1216</td>\n",
       "      <td>1264</td>\n",
       "      <td>I have no option to add the one you have sent me</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     score  start   end                                            answer\n",
       "0  0.00029   1216  1264  I have no option to add the one you have sent me"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_a = pipeline('question-answering')\n",
    "question = \"What the problem with IHS ?\"\n",
    "output = q_a(question=question, context=text)\n",
    "pd.DataFrame([output])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df46d03b",
   "metadata": {},
   "source": [
    "## Summarization\n",
    "But what if we have a long text, or document that we need to summarize to get a quick overview about that text.\n",
    "\n",
    "The goal of text summarization is to take a long text as input and generate a short version with all the relevant facts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "563917f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " I have paid out the Immigration Health Surcharge with this reference number \"IHS109092150PA01\", and the IHS sent me on Feb 7, 2023, top-up payment is needed. The transaction reference for your payment of USD 3\n"
     ]
    }
   ],
   "source": [
    "summarizer = pipeline('summarization')\n",
    "output = summarizer(text, max_length=56, clean_up_tokenization_spaces=True)\n",
    "print(output[0]['summary_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc2c095",
   "metadata": {},
   "source": [
    "## Translation\n",
    "What if you are not understand English, then you can tralsate the feedback into your lanugage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d6942d31",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (â€¦)olve/main/source.spm: 100%|â–ˆâ–ˆâ–ˆâ–ˆ| 768k/768k [00:02<00:00, 299kB/s]\n",
      "Downloading (â€¦)olve/main/target.spm: 100%|â–ˆâ–ˆâ–ˆâ–ˆ| 797k/797k [00:02<00:00, 325kB/s]\n",
      "Downloading (â€¦)olve/main/vocab.json: 100%|â–ˆâ–ˆ| 1.27M/1.27M [00:03<00:00, 416kB/s]\n",
      "/home/abdelrahman/.local/lib/python3.8/site-packages/transformers/models/marian/tokenization_marian.py:194: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ich habe die Immigration Health Surcharge mit dieser Referenznummer \"IHS109092150PA01\" bezahlt, und die IHS schickte mir eine E-Mail \"Die Transaktion Referenz fÃ¼r Ihre Zahlung von USD 3.950.41 ist IHS109092150PA01 -2023-01-26T11:05:00. Nach Abschluss Ihrer IHS-Anwendung werden Sie auch eine Kopie Ihrer IHS Referenznummer, die Sie fÃ¼r Ihre Einwanderung Anwendung benÃ¶tigen per E-Mail.' Ich habe einen Screenshot der Zahlung. Aber die Immigration Gesundheitszuschlag schickt mich am 7. Februar 2023, Nachzahlung erforderlich ist und das E-Mail-Format ist unten angegeben: \"Name: MR Abdelrahman Hamdy Rezk IHS Referenznummer: IHS017333073 Eine Ã„nderung wurde an die Details, die Sie angegeben. Sie mÃ¼ssen eine zusÃ¤tzliche Zahlung von 3964.2 (USD) bis 14 Februar 2023 zahlen. \"Ich habe einen Screenshot der E-Mail beigefÃ¼gt. Also, Sie haben mich auf diese Referenznummer bezahlen lassen.\n"
     ]
    }
   ],
   "source": [
    "translator = pipeline(\"translation_en_to_de\", model=\"Helsinki-NLP/opus-mt-en-de\")\n",
    "outputs = translator(text, clean_up_tokenization_spaces=True, min_length=100)\n",
    "print(outputs[0]['translation_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ad330d",
   "metadata": {},
   "source": [
    "## Text Generation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "adbeb933",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to gpt2 and revision 6c0e608 (https://huggingface.co/gpt2).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Downloading (â€¦)lve/main/config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 665/665 [00:00<00:00, 213kB/s]\n",
      "Downloading (â€¦)\"pytorch_model.bin\";:  78%|â–ˆâ–ˆâ–Ž| 430M/548M [04:05<01:07, 1.75MB/s]\n",
      "Downloading (â€¦)\"tf_model.h5\";: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 498M/498M [04:23<00:00, 1.89MB/s]\n",
      "2023-02-28 13:11:43.131333: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-02-28 13:11:43.131979: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-02-28 13:11:43.132484: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (abdelrahman-ThinkPad-X1-Carbon-3rd): /proc/driver/nvidia/version does not exist\n",
      "2023-02-28 13:11:43.140682: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-28 13:11:43.364827: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 154389504 exceeds 10% of free system memory.\n",
      "2023-02-28 13:11:43.711711: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 154389504 exceeds 10% of free system memory.\n",
      "2023-02-28 13:11:43.737072: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 154389504 exceeds 10% of free system memory.\n",
      "2023-02-28 13:11:45.202430: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 154389504 exceeds 10% of free system memory.\n",
      "2023-02-28 13:11:45.563641: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 154389504 exceeds 10% of free system memory.\n",
      "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
      "\n",
      "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at gpt2.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n",
      "Downloading (â€¦)neration_config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 124/124 [00:00<00:00, 58.1kB/s]\n",
      "Downloading (â€¦)olve/main/vocab.json: 100%|â–ˆâ–ˆ| 1.04M/1.04M [00:06<00:00, 162kB/s]\n",
      "Downloading (â€¦)olve/main/merges.txt: 100%|â–ˆâ–ˆâ–ˆâ–ˆ| 456k/456k [00:01<00:00, 271kB/s]\n",
      "Downloading (â€¦)/main/tokenizer.json: 100%|â–ˆâ–ˆ| 1.36M/1.36M [00:03<00:00, 340kB/s]\n",
      "/home/abdelrahman/.local/lib/python3.8/site-packages/transformers/generation/tf_utils.py:603: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Input length of input_ids is 392, but `max_length` is set to 200. This can lead to unexpected behavior. You should consider increasing`max_new_tokens`.\n",
      "2023-02-28 13:12:20.377801: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at constant_op.cc:171 : INVALID_ARGUMENT: Dimension -192 must be >= 0\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "{{function_node __wrapped__Fill_device_/job:localhost/replica:0/task:0/device:CPU:0}} Dimension -192 must be >= 0 [Op:Fill]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDear Bumblebee, I am sorry to hear that your order was mixed up.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      3\u001b[0m prompt \u001b[38;5;241m=\u001b[39m text \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mCustomer service response:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m response\n\u001b[0;32m----> 4\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mgenerator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(outputs[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgenerated_text\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/pipelines/text_generation.py:210\u001b[0m, in \u001b[0;36mTextGenerationPipeline.__call__\u001b[0;34m(self, text_inputs, **kwargs)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, text_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    170\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;124;03m    Complete the prompt(s) given as inputs.\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;124;03m          ids of the generated text.\u001b[39;00m\n\u001b[1;32m    209\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 210\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtext_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/pipelines/base.py:1084\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1076\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\n\u001b[1;32m   1077\u001b[0m         \u001b[38;5;28miter\u001b[39m(\n\u001b[1;32m   1078\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_iterator(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1081\u001b[0m         )\n\u001b[1;32m   1082\u001b[0m     )\n\u001b[1;32m   1083\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1084\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpostprocess_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/pipelines/base.py:1091\u001b[0m, in \u001b[0;36mPipeline.run_single\u001b[0;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[1;32m   1089\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_single\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[1;32m   1090\u001b[0m     model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess(inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpreprocess_params)\n\u001b[0;32m-> 1091\u001b[0m     model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1092\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpostprocess(model_outputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpostprocess_params)\n\u001b[1;32m   1093\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/pipelines/base.py:987\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m    985\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    986\u001b[0m     model_inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 987\u001b[0m     model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    988\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    989\u001b[0m     inference_context \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_inference_context()\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/pipelines/text_generation.py:252\u001b[0m, in \u001b[0;36mTextGenerationPipeline._forward\u001b[0;34m(self, model_inputs, **generate_kwargs)\u001b[0m\n\u001b[1;32m    250\u001b[0m prompt_text \u001b[38;5;241m=\u001b[39m model_inputs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt_text\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    251\u001b[0m \u001b[38;5;66;03m# BS x SL\u001b[39;00m\n\u001b[0;32m--> 252\u001b[0m generated_sequence \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgenerate_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    253\u001b[0m out_b \u001b[38;5;241m=\u001b[39m generated_sequence\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/generation/tf_utils.py:809\u001b[0m, in \u001b[0;36mTFGenerationMixin.generate\u001b[0;34m(self, input_ids, generation_config, seed, **kwargs)\u001b[0m\n\u001b[1;32m    801\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m    802\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m    803\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m    804\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m    805\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m    806\u001b[0m     )\n\u001b[1;32m    808\u001b[0m     \u001b[38;5;66;03m# 13. run sample\u001b[39;00m\n\u001b[0;32m--> 809\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    810\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    811\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    812\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_warper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits_warper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    813\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    814\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    815\u001b[0m \u001b[43m        \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    816\u001b[0m \u001b[43m        \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    817\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_scores\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_scores\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    818\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_dict_in_generate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    819\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    820\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_beam_gen_mode:\n\u001b[1;32m    823\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m generation_config\u001b[38;5;241m.\u001b[39mnum_beams \u001b[38;5;241m<\u001b[39m generation_config\u001b[38;5;241m.\u001b[39mnum_return_sequences:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/generation/tf_utils.py:1638\u001b[0m, in \u001b[0;36mTFGenerationMixin.sample\u001b[0;34m(self, input_ids, logits_processor, logits_warper, max_length, pad_token_id, eos_token_id, seed, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, **model_kwargs)\u001b[0m\n\u001b[1;32m   1635\u001b[0m batch_size, cur_len \u001b[38;5;241m=\u001b[39m shape_list(input_ids)\n\u001b[1;32m   1637\u001b[0m \u001b[38;5;66;03m# initialize `generated` (pre-populated with `pad_token_id`), `finished_sequences`\u001b[39;00m\n\u001b[0;32m-> 1638\u001b[0m input_ids_padding \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mones\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcur_len\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mint32\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m (pad_token_id \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m   1639\u001b[0m generated \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconcat([input_ids, input_ids_padding], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m   1640\u001b[0m finished_sequences \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mzeros((batch_size,), dtype\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mbool)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/framework/ops.py:7215\u001b[0m, in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   7213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mraise_from_not_ok_status\u001b[39m(e, name):\n\u001b[1;32m   7214\u001b[0m   e\u001b[38;5;241m.\u001b[39mmessage \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m name: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m name \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 7215\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: {{function_node __wrapped__Fill_device_/job:localhost/replica:0/task:0/device:CPU:0}} Dimension -192 must be >= 0 [Op:Fill]"
     ]
    }
   ],
   "source": [
    "generator = pipeline(\"text-generation\")\n",
    "response = \"Dear Abdelrahman, I am sorry to hear that your IHS was wrongly assigned to wrong application.\"\n",
    "prompt = text + \"\\n\\nCustomer service response:\\n\" + response\n",
    "outputs = generator(prompt, max_length=200)\n",
    "print(outputs[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fab2974",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb66f55",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
