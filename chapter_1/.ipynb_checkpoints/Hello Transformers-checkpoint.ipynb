{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e2b18b98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.1.97-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m[36m0:00:01\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0m\n",
      "\u001b[?25hInstalling collected packages: sentencepiece\n",
      "Successfully installed sentencepiece-0.1.97\n"
     ]
    }
   ],
   "source": [
    "# !pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f3b3a53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abdelrahman/.local/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-02-28 12:58:11.141588: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-28 12:58:11.472684: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-02-28 12:58:11.472721: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-02-28 12:58:12.636164: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-28 12:58:12.636255: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-02-28 12:58:12.636265: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ebc3877",
   "metadata": {},
   "source": [
    "## A Tour of Transformer Applications\n",
    "\n",
    "We will see some progress bars as hugging face will start to install the model weights into our local machine from the Hugging Face Hub.\n",
    "The second time you instantiate the pipeline, the library will notice that you’ve already downloaded the weights and will use the cached version instead.\n",
    "\n",
    "If no frame work is specified the hugging face will install the original framework used with the model, other wise you can use the framework parameter to specify the framework you need to be used, and this is good because it maybe install the two versions of the model with pytorch or tensorflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10c0b2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"I have paid out the Immigration Health Surcharge with this reference number \"IHS109092150PA01\", and the IHS sent me an email 'The transaction reference for your payment of USD 3,950.41 is IHS109092150PA01|2023-01-26T11:05:00. On completion of your IHS application you will also be emailed a copy of your IHS reference number, which you will need for your immigration application.'\n",
    "\n",
    "I have attached a screenshot of the payment.\n",
    "\n",
    "\n",
    "But the Immigration health surcharge sends me on Feb 7, 2023, top-up payment is needed and the email format is quoted below:\n",
    "\n",
    "'Name: MR Abdelrahman Hamdy Rezk\n",
    "\n",
    "IHS reference number: IHS017333073\n",
    "\n",
    "A change has been made to the details you provided. You need to pay an additional payment of 3964.2 (USD) by 14 February 2023.'\n",
    "\n",
    "I have attached a screenshot of the email.\n",
    "\n",
    "\n",
    "So, you have asked me to pay on this reference number: IHS017333073 which has also been on my application, but I have paid out and you have sent me an email with the payment with this reference number: IHS109092150. So please try to solve this problem because they asked me to pay by 14 February 2023, and I already paid, and you have directed me to the next step of my application with another reference number, and I have no option to add the one you have sent me after the IHS payment, so you have to change my payment number of IHS to the IHS number mentioned in my application and you have asked me to pay for it.\n",
    "\n",
    "I have attached all screenshots required.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576cc6ad",
   "metadata": {},
   "source": [
    "# Text Classification\n",
    "\n",
    "Most of the NLP applications start to know your customer opinion about your services or anything, know the people's sentiments about anything is very helpful, and it takes no time to know the sentiment of some text.\n",
    "This can be done in just few lines of code using Huggingface transformers, but we will go deeper o fine-tune these models in chapter 2.\n",
    "\n",
    "Huggingface Transformers allow you to use its apis at various level of abstraction as you need:\n",
    "- Direct use of the model (As we use the pipeline for the task directly)\n",
    "- Build your own model\n",
    "- load the model in your perfared framework\n",
    "- Fine tune the model\n",
    "\n",
    "The pipelines is abstract away all the steps needed to convert raw text into a set of predictions from a fine-tuned model.\n",
    "\n",
    "pipelines is used by provide the nlp task you need to done on the provided text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bdc45e0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td>0.997495</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      label     score\n",
       "0  NEGATIVE  0.997495"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = pipeline(\"text-classification\")\n",
    "# We can use list of texts, and it will also return a list of predictions, and use DataFrame to be more visible\n",
    "outputs = classifier(text)\n",
    "pd.DataFrame(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a4d5386",
   "metadata": {},
   "source": [
    "## Named Entity Recognition\n",
    "\n",
    "Now we have known that its a negative comment from one of our customer, and we need to know what is the complains is about, we need to know much more information instead of reading this long complains, we need to know this compain is about a particular item or service, this known as NER, which extract the objects and classify into some of the categories like its about place or product or services and so on.\n",
    "\n",
    "We can see how the model extracts the objects from the provided text, as well as give it an entity, and aggregation_strategy simple is to use the categories defined by the model.\n",
    "\n",
    "The scores tell us how confident the model was about the entities it identified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e615f283",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision f2482bf (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>entity_group</th>\n",
       "      <th>score</th>\n",
       "      <th>word</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ORG</td>\n",
       "      <td>0.846629</td>\n",
       "      <td>I</td>\n",
       "      <td>77</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ORG</td>\n",
       "      <td>0.996680</td>\n",
       "      <td>IHS</td>\n",
       "      <td>104</td>\n",
       "      <td>107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ORG</td>\n",
       "      <td>0.974536</td>\n",
       "      <td>IHS</td>\n",
       "      <td>248</td>\n",
       "      <td>251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ORG</td>\n",
       "      <td>0.978925</td>\n",
       "      <td>IHS</td>\n",
       "      <td>304</td>\n",
       "      <td>307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ORG</td>\n",
       "      <td>0.959406</td>\n",
       "      <td>Immigration</td>\n",
       "      <td>437</td>\n",
       "      <td>448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>PER</td>\n",
       "      <td>0.890806</td>\n",
       "      <td>Abdelrahman Hamdy Rezk</td>\n",
       "      <td>565</td>\n",
       "      <td>587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ORG</td>\n",
       "      <td>0.936408</td>\n",
       "      <td>IHS</td>\n",
       "      <td>589</td>\n",
       "      <td>592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ORG</td>\n",
       "      <td>0.916784</td>\n",
       "      <td>IHS</td>\n",
       "      <td>1275</td>\n",
       "      <td>1278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ORG</td>\n",
       "      <td>0.995584</td>\n",
       "      <td>IHS</td>\n",
       "      <td>1331</td>\n",
       "      <td>1334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ORG</td>\n",
       "      <td>0.943982</td>\n",
       "      <td>IHS</td>\n",
       "      <td>1342</td>\n",
       "      <td>1345</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  entity_group     score                    word  start   end\n",
       "0          ORG  0.846629                       I     77    78\n",
       "1          ORG  0.996680                     IHS    104   107\n",
       "2          ORG  0.974536                     IHS    248   251\n",
       "3          ORG  0.978925                     IHS    304   307\n",
       "4          ORG  0.959406             Immigration    437   448\n",
       "5          PER  0.890806  Abdelrahman Hamdy Rezk    565   587\n",
       "6          ORG  0.936408                     IHS    589   592\n",
       "7          ORG  0.916784                     IHS   1275  1278\n",
       "8          ORG  0.995584                     IHS   1331  1334\n",
       "9          ORG  0.943982                     IHS   1342  1345"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Same we use pipeline but we have point to another nlp task which is ner\n",
    "ner_tagger = pipeline('ner', aggregation_strategy=\"simple\")\n",
    "outputs = ner_tagger(text)\n",
    "pd.DataFrame(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b81a4d",
   "metadata": {},
   "source": [
    "## Question Answering\n",
    "Extracting all the named entities in a text is nice, but sometimes we would like to ask more targeted questions. This is where we can use question answering.\n",
    "\n",
    "We provide the model with the context(our text) along with questions we need to get answers for this text.\n",
    "\n",
    "Some questions and answering systems have extracted the answer from the text itself and its called extractive question answering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "835597f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-cased-distilled-squad and revision 626af31 (https://huggingface.co/distilbert-base-cased-distilled-squad).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00029</td>\n",
       "      <td>1216</td>\n",
       "      <td>1264</td>\n",
       "      <td>I have no option to add the one you have sent me</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     score  start   end                                            answer\n",
       "0  0.00029   1216  1264  I have no option to add the one you have sent me"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_a = pipeline('question-answering')\n",
    "question = \"What the problem with IHS ?\"\n",
    "output = q_a(question=question, context=text)\n",
    "pd.DataFrame([output])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320e58c2",
   "metadata": {},
   "source": [
    "## Summarization\n",
    "But what if we have a long text, or document that we need to summarize to get a quick overview about that text.\n",
    "\n",
    "The goal of text summarization is to take a long text as input and generate a short version with all the relevant facts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "563917f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " I have paid out the Immigration Health Surcharge with this reference number \"IHS109092150PA01\", and the IHS sent me on Feb 7, 2023, top-up payment is needed. The transaction reference for your payment of USD 3\n"
     ]
    }
   ],
   "source": [
    "summarizer = pipeline('summarization')\n",
    "output = summarizer(text, max_length=56, clean_up_tokenization_spaces=True)\n",
    "print(output[0]['summary_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2668dd0f",
   "metadata": {},
   "source": [
    "## Translation\n",
    "What if you are not understand English, then you can tralsate the feedback into your lanugage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d6942d31",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)lve/main/config.json: 100%|██| 1.33k/1.33k [00:00<00:00, 498kB/s]\n",
      "Downloading (…)\"pytorch_model.bin\";: 100%|███| 298M/298M [02:28<00:00, 2.01MB/s]\n",
      "Downloading (…)neration_config.json: 100%|█████| 293/293 [00:00<00:00, 77.1kB/s]\n",
      "Downloading (…)okenizer_config.json: 100%|███| 42.0/42.0 [00:00<00:00, 9.06kB/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "This tokenizer cannot be instantiated. Please make sure you have `sentencepiece` installed in order to use this tokenizer.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m translator \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtranslation_en_to_de\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHelsinki-NLP/opus-mt-en-de\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m outputs \u001b[38;5;241m=\u001b[39m translator(text, clean_up_tokenization_spaces\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, min_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(outputs[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtranslation_text\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/pipelines/__init__.py:828\u001b[0m, in \u001b[0;36mpipeline\u001b[0;34m(task, model, config, tokenizer, feature_extractor, framework, revision, use_fast, use_auth_token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[1;32m    825\u001b[0m             tokenizer_identifier \u001b[38;5;241m=\u001b[39m tokenizer\n\u001b[1;32m    826\u001b[0m             tokenizer_kwargs \u001b[38;5;241m=\u001b[39m model_kwargs\n\u001b[0;32m--> 828\u001b[0m         tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mAutoTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    829\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtokenizer_identifier\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_fast\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_fast\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_from_pipeline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtokenizer_kwargs\u001b[49m\n\u001b[1;32m    830\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m load_feature_extractor:\n\u001b[1;32m    833\u001b[0m     \u001b[38;5;66;03m# Try to infer feature extractor from model or config name (if provided as str)\u001b[39;00m\n\u001b[1;32m    834\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m feature_extractor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/transformers/models/auto/tokenization_auto.py:681\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    679\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer_class_py\u001b[38;5;241m.\u001b[39mfrom_pretrained(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    680\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 681\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    682\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis tokenizer cannot be instantiated. Please make sure you have `sentencepiece` installed \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    683\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min order to use this tokenizer.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    684\u001b[0m             )\n\u001b[1;32m    686\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    687\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m to build an AutoTokenizer.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    688\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mTOKENIZER_MAPPING\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    689\u001b[0m )\n",
      "\u001b[0;31mValueError\u001b[0m: This tokenizer cannot be instantiated. Please make sure you have `sentencepiece` installed in order to use this tokenizer."
     ]
    }
   ],
   "source": [
    "translator = pipeline(\"translation_en_to_de\", model=\"Helsinki-NLP/opus-mt-en-de\")\n",
    "outputs = translator(text, clean_up_tokenization_spaces=True, min_length=100)\n",
    "print(outputs[0]['translation_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23547541",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adbeb933",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = pipeline(\"text-generation\")\n",
    "response = \"Dear Bumblebee, I am sorry to hear that your order was mixed up.\"\n",
    "prompt = text + \"\\n\\nCustomer service response:\\n\" + response\n",
    "outputs = generator(prompt, max_length=200)\n",
    "print(outputs[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fab2974",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb66f55",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
