{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6560ce1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install transformers\n",
    "# !pip3 install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89dec069",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, list_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd43b634",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "function"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(list_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e6cb751b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of Avaliable datasets and it can be increased as based on datasets version:  4651\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['acronym_identification',\n",
       " 'ade_corpus_v2',\n",
       " 'adversarial_qa',\n",
       " 'aeslc',\n",
       " 'afrikaans_ner_corpus',\n",
       " 'ag_news',\n",
       " 'ai2_arc',\n",
       " 'air_dialogue',\n",
       " 'ajgt_twitter_ar',\n",
       " 'allegro_reviews']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_datasets = list_datasets()\n",
    "print(\"The number of Avaliable datasets and it can be increased as based on datasets version: \", len(all_datasets))\n",
    "all_datasets[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c35e354",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading builder script: 3.62kB [00:00, 1.35MB/s]                   \n",
      "Downloading metadata: 3.28kB [00:00, 1.18MB/s]                   \n",
      "Using custom data configuration default\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset emotion/default (download: 1.97 MiB, generated: 2.07 MiB, post-processed: Unknown size, total: 4.05 MiB) to /home/qblocks/.cache/huggingface/datasets/emotion/default/0.0.0/348f63ca8e27b3713b6c04d723efe6d824a56fb3d1449794716c0f0296072705...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 1.66M/1.66M [00:00<00:00, 10.7MB/s]\n",
      "Downloading data: 100%|██████████| 204k/204k [00:00<00:00, 2.66MB/s]\n",
      "Downloading data: 100%|██████████| 207k/207k [00:00<00:00, 2.69MB/s]\n",
      "                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset emotion downloaded and prepared to /home/qblocks/.cache/huggingface/datasets/emotion/default/0.0.0/348f63ca8e27b3713b6c04d723efe6d824a56fb3d1449794716c0f0296072705. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 491.46it/s]\n"
     ]
    }
   ],
   "source": [
    "# lets use emotion dataset\n",
    "# it takes a time to load the dataset for first time, then it use the cassed version next time you run the code\n",
    "emtions = load_dataset('emotion')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d896f9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datasets.dataset_dict.DatasetDict"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(emtions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0f4b5154",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 16000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emtions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f9ddcd58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'datasets.arrow_dataset.Dataset'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label'],\n",
       "    num_rows: 16000\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# looks like dictionary So !\n",
    "train_data = emtions['train']\n",
    "print(type(train_data))\n",
    "train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1388a35e",
   "metadata": {},
   "source": [
    "# Dataset Object\n",
    "\n",
    "This class of **Dataset** one of the strong data structure in the dataset library, its like python list, so we can access using indexed way. \n",
    "This **Dataset** data structure based on Appache arrow which defines type of column format than more memory effient than python code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a34b774b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'text': ['i didnt feel humiliated',\n",
       "  'i can go from feeling so hopeless to so damned hopeful just from being around someone who cares and is awake',\n",
       "  'im grabbing a minute to post i feel greedy wrong'],\n",
       " 'label': [0, 0, 3]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(type(train_data[:3]))\n",
    "\n",
    "train_data[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bda3c93b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 3]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if we just interested in just the labels\n",
    "train_data['label'][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "36e0cd41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['text', 'label']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.column_names # as like data frame get the different columns in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d04ebcf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': Value(dtype='string', id=None),\n",
       " 'label': ClassLabel(num_classes=6, names=['sadness', 'joy', 'love', 'anger', 'fear', 'surprise'], id=None)}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# As saw above this Dataset object has features key and it hold information about each col datatype\n",
    "train_data.features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10880fdc",
   "metadata": {},
   "source": [
    "# load_dataset\n",
    "\n",
    "Its not only about the dataset available on Huggin hub, its about other data either on your machine of anywhere, it also can handle different types of the dataset like csv and other.\n",
    "\n",
    "The load_dataset use the corresponding  script file for your datafile to load it.\n",
    "\n",
    "**Lets get emotion dataset from its source**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9d7c2fbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-05-07 08:42:59--  https://www.dropbox.com/s/1pzkadrvffbqw6o/train.txt\n",
      "Resolving www.dropbox.com (www.dropbox.com)... 162.125.8.18, 2620:100:601b:18::a27d:812\n",
      "Connecting to www.dropbox.com (www.dropbox.com)|162.125.8.18|:443... connected.\n",
      "HTTP request sent, awaiting response... 301 Moved Permanently\n",
      "Location: /s/raw/1pzkadrvffbqw6o/train.txt [following]\n",
      "--2022-05-07 08:42:59--  https://www.dropbox.com/s/raw/1pzkadrvffbqw6o/train.txt\n",
      "Reusing existing connection to www.dropbox.com:443.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://uc43d0393907b953e314959bfaf5.dl.dropboxusercontent.com/cd/0/inline/Bkwn1uMI2LPK3mboUjsYsDhzRNUvEYuCx0gaFpP7emfqRffHdnPLIXu77TQ3_zBz8ZdD41FG4-K4WTBrARQ-paiOMzg78sYdn2EAoqnd8TsDj0enHYVIB9e9m9Ded-0Rx7NAuKwYiYgbQj8n-XA-EAPTuOKPM_yFSFCuyDPAd74HuA/file# [following]\n",
      "--2022-05-07 08:42:59--  https://uc43d0393907b953e314959bfaf5.dl.dropboxusercontent.com/cd/0/inline/Bkwn1uMI2LPK3mboUjsYsDhzRNUvEYuCx0gaFpP7emfqRffHdnPLIXu77TQ3_zBz8ZdD41FG4-K4WTBrARQ-paiOMzg78sYdn2EAoqnd8TsDj0enHYVIB9e9m9Ded-0Rx7NAuKwYiYgbQj8n-XA-EAPTuOKPM_yFSFCuyDPAd74HuA/file\n",
      "Resolving uc43d0393907b953e314959bfaf5.dl.dropboxusercontent.com (uc43d0393907b953e314959bfaf5.dl.dropboxusercontent.com)... 162.125.8.15, 2620:100:601b:15::a27d:80f\n",
      "Connecting to uc43d0393907b953e314959bfaf5.dl.dropboxusercontent.com (uc43d0393907b953e314959bfaf5.dl.dropboxusercontent.com)|162.125.8.15|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1658616 (1.6M) [text/plain]\n",
      "Saving to: ‘train.txt.1’\n",
      "\n",
      "train.txt.1         100%[===================>]   1.58M  10.2MB/s    in 0.2s    \n",
      "\n",
      "2022-05-07 08:43:00 (10.2 MB/s) - ‘train.txt.1’ saved [1658616/1658616]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_url = \"https://www.dropbox.com/s/1pzkadrvffbqw6o/train.txt\"\n",
    "!wget {data_url}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b93ee5ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/qblocks/nlp_with_transformers/chapter_2\n"
     ]
    }
   ],
   "source": [
    "!pwd train.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e4ebcb7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i didnt feel humiliated;sadness\n",
      "i can go from feeling so hopeless to so damned hopeful just from being around someone who cares and is awake;sadness\n",
      "im grabbing a minute to post i feel greedy wrong;anger\n",
      "i am ever feeling nostalgic about the fireplace i will know that it is still on the property;love\n",
      "i am feeling grouchy;anger\n"
     ]
    }
   ],
   "source": [
    "!head -5 train.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab43fed",
   "metadata": {},
   "source": [
    "# lets load that file into dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4cf14eb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m\n",
       "\u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mpath\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdata_dir\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdata_files\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0msplit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSplit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mcache_dir\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mfeatures\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdownload_config\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDownloadConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdownload_mode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDownloadMode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mignore_verifications\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mkeep_in_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0msave_infos\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mrevision\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVersion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0muse_auth_token\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtask\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTaskTemplate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mstreaming\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m**\u001b[0m\u001b[0mconfig_kwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDatasetDict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marrow_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterableDatasetDict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterable_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterableDataset\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mSource:\u001b[0m   \n",
       "\u001b[0;32mdef\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mpath\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdata_dir\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdata_files\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0msplit\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSplit\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mcache_dir\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mfeatures\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mFeatures\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdownload_config\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDownloadConfig\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdownload_mode\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDownloadMode\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mignore_verifications\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mkeep_in_memory\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0msave_infos\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mrevision\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVersion\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0muse_auth_token\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtask\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTaskTemplate\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mstreaming\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m**\u001b[0m\u001b[0mconfig_kwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDatasetDict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIterableDatasetDict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIterableDataset\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m\"\"\"Load a dataset from the Hugging Face Hub, or a local dataset.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    You can find the list of datasets on the Hub at https://huggingface.co/datasets or with ``datasets.list_datasets()``.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    A dataset is a directory that contains:\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    - some data files in generic formats (JSON, CSV, Parquet, text, etc.)\u001b[0m\n",
       "\u001b[0;34m    - and optionally a dataset script, if it requires some code to read the data files. This is used to load any kind of formats or structures.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    Note that dataset scripts can also download and read data files from anywhere - in case your data files already exist online.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    This function does the following under the hood:\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m        1. Download and import in the library the dataset script from ``path`` if it's not already cached inside the library.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m            If the dataset has no dataset script, then a generic dataset script is imported instead (JSON, CSV, Parquet, text, etc.)\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m            Dataset scripts are small python scripts that define dataset builders. They define the citation, info and format of the dataset,\u001b[0m\n",
       "\u001b[0;34m            contain the path or URL to the original data files and the code to load examples from the original data files.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m            You can find some of the scripts here: https://github.com/huggingface/datasets/tree/master/datasets\u001b[0m\n",
       "\u001b[0;34m            You can find the complete list of datasets in the Datasets Hub at https://huggingface.co/datasets\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m        2. Run the dataset script which will:\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m            * Download the dataset file from the original URL (see the script) if it's not already available locally or cached.\u001b[0m\n",
       "\u001b[0;34m            * Process and cache the dataset in typed Arrow tables for caching.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m                Arrow table are arbitrarily long, typed tables which can store nested objects and be mapped to numpy/pandas/python generic types.\u001b[0m\n",
       "\u001b[0;34m                They can be directly accessed from disk, loaded in RAM or even streamed over the web.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m        3. Return a dataset built from the requested splits in ``split`` (default: all).\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    It also allows to load a dataset from a local directory or a dataset repository on the Hugging Face Hub without dataset script.\u001b[0m\n",
       "\u001b[0;34m    In this case, it automatically loads all the data files from the directory or the dataset repository.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    Args:\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m        path (:obj:`str`): Path or name of the dataset.\u001b[0m\n",
       "\u001b[0;34m            Depending on ``path``, the dataset builder that is used comes from a generic dataset script (JSON, CSV, Parquet, text etc.) or from the dataset script (a python file) inside the dataset directory.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m            For local datasets:\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m            - if ``path`` is a local directory (containing data files only)\u001b[0m\n",
       "\u001b[0;34m              -> load a generic dataset builder (csv, json, text etc.) based on the content of the directory\u001b[0m\n",
       "\u001b[0;34m              e.g. ``'./path/to/directory/with/my/csv/data'``.\u001b[0m\n",
       "\u001b[0;34m            - if ``path`` is a local dataset script or a directory containing a local dataset script (if the script has the same name as the directory):\u001b[0m\n",
       "\u001b[0;34m              -> load the dataset builder from the dataset script\u001b[0m\n",
       "\u001b[0;34m              e.g. ``'./dataset/squad'`` or ``'./dataset/squad/squad.py'``.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m            For datasets on the Hugging Face Hub (list all available datasets and ids with ``datasets.list_datasets()``)\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m            - if ``path`` is a dataset repository on the HF hub (containing data files only)\u001b[0m\n",
       "\u001b[0;34m              -> load a generic dataset builder (csv, text etc.) based on the content of the repository\u001b[0m\n",
       "\u001b[0;34m              e.g. ``'username/dataset_name'``, a dataset repository on the HF hub containing your data files.\u001b[0m\n",
       "\u001b[0;34m            - if ``path`` is a dataset repository on the HF hub with a dataset script (if the script has the same name as the directory)\u001b[0m\n",
       "\u001b[0;34m              -> load the dataset builder from the dataset script in the dataset repository\u001b[0m\n",
       "\u001b[0;34m              e.g. ``glue``, ``squad``, ``'username/dataset_name'``, a dataset repository on the HF hub containing a dataset script `'dataset_name.py'`.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m        name (:obj:`str`, optional): Defining the name of the dataset configuration.\u001b[0m\n",
       "\u001b[0;34m        data_dir (:obj:`str`, optional): Defining the data_dir of the dataset configuration. If specified for the generic builders (csv, text etc.) or the Hub datasets and `data_files` is None,\u001b[0m\n",
       "\u001b[0;34m            the behavior is equal to passing `os.path.join(data_dir, **)` as `data_files` to reference all the files in a directory.\u001b[0m\n",
       "\u001b[0;34m        data_files (:obj:`str` or :obj:`Sequence` or :obj:`Mapping`, optional): Path(s) to source data file(s).\u001b[0m\n",
       "\u001b[0;34m        split (:class:`Split` or :obj:`str`): Which split of the data to load.\u001b[0m\n",
       "\u001b[0;34m            If None, will return a `dict` with all splits (typically `datasets.Split.TRAIN` and `datasets.Split.TEST`).\u001b[0m\n",
       "\u001b[0;34m            If given, will return a single Dataset.\u001b[0m\n",
       "\u001b[0;34m            Splits can be combined and specified like in tensorflow-datasets.\u001b[0m\n",
       "\u001b[0;34m        cache_dir (:obj:`str`, optional): Directory to read/write data. Defaults to \"~/.cache/huggingface/datasets\".\u001b[0m\n",
       "\u001b[0;34m        features (:class:`Features`, optional): Set the features type to use for this dataset.\u001b[0m\n",
       "\u001b[0;34m        download_config (:class:`~utils.DownloadConfig`, optional): Specific download configuration parameters.\u001b[0m\n",
       "\u001b[0;34m        download_mode (:class:`DownloadMode`, default ``REUSE_DATASET_IF_EXISTS``): Download/generate mode.\u001b[0m\n",
       "\u001b[0;34m        ignore_verifications (:obj:`bool`, default ``False``): Ignore the verifications of the downloaded/processed dataset information (checksums/size/splits/...).\u001b[0m\n",
       "\u001b[0;34m        keep_in_memory (:obj:`bool`, default ``None``): Whether to copy the dataset in-memory. If `None`, the dataset\u001b[0m\n",
       "\u001b[0;34m            will not be copied in-memory unless explicitly enabled by setting `datasets.config.IN_MEMORY_MAX_SIZE` to\u001b[0m\n",
       "\u001b[0;34m            nonzero. See more details in the :ref:`load_dataset_enhancing_performance` section.\u001b[0m\n",
       "\u001b[0;34m        save_infos (:obj:`bool`, default ``False``): Save the dataset information (checksums/size/splits/...).\u001b[0m\n",
       "\u001b[0;34m        revision (:class:`~utils.Version` or :obj:`str`, optional): Version of the dataset script to load:\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m            - For datasets in the `huggingface/datasets` library on GitHub like \"squad\", the default version of the module is the local version of the lib.\u001b[0m\n",
       "\u001b[0;34m              You can specify a different version from your local version of the lib (e.g. \"master\" or \"1.2.0\") but it might cause compatibility issues.\u001b[0m\n",
       "\u001b[0;34m            - For community datasets like \"lhoestq/squad\" that have their own git repository on the Datasets Hub, the default version \"main\" corresponds to the \"main\" branch.\u001b[0m\n",
       "\u001b[0;34m              You can specify a different version that the default \"main\" by using a commit sha or a git tag of the dataset repository.\u001b[0m\n",
       "\u001b[0;34m        use_auth_token (``str`` or :obj:`bool`, optional): Optional string or boolean to use as Bearer token for remote files on the Datasets Hub.\u001b[0m\n",
       "\u001b[0;34m            If True, will get token from `\"~/.huggingface\"`.\u001b[0m\n",
       "\u001b[0;34m        task (``str``): The task to prepare the dataset for during training and evaluation. Casts the dataset's :class:`Features` to standardized column names and types as detailed in :py:mod:`datasets.tasks`.\u001b[0m\n",
       "\u001b[0;34m        streaming (:obj:`bool`, default ``False``): If set to True, don't download the data files. Instead, it streams the data progressively while\u001b[0m\n",
       "\u001b[0;34m            iterating on the dataset. An IterableDataset or IterableDatasetDict is returned instead in this case.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m            Note that streaming works for datasets that use data formats that support being iterated over like txt, csv, jsonl for example.\u001b[0m\n",
       "\u001b[0;34m            Json files may be downloaded completely. Also streaming from remote zip or gzip files is supported but other compressed formats\u001b[0m\n",
       "\u001b[0;34m            like rar and xz are not yet supported. The tgz format doesn't allow streaming.\u001b[0m\n",
       "\u001b[0;34m        **config_kwargs: Keyword arguments to be passed to the :class:`BuilderConfig` and used in the :class:`DatasetBuilder`.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    Returns:\u001b[0m\n",
       "\u001b[0;34m        :class:`Dataset` or :class:`DatasetDict`:\u001b[0m\n",
       "\u001b[0;34m        - if `split` is not None: the dataset requested,\u001b[0m\n",
       "\u001b[0;34m        - if `split` is None, a ``datasets.DatasetDict`` with each split.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m        or :class:`IterableDataset` or :class:`IterableDatasetDict`: if streaming=True\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m        - if `split` is not None: the dataset requested,\u001b[0m\n",
       "\u001b[0;34m        - if `split` is None, a ``datasets.streaming.IterableDatasetDict`` with each split.\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m    \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mif\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDATASET_STATE_JSON_FILENAME\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;34m\"You are trying to load a dataset that was saved using `save_to_disk`. \"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0;34m\"Please use `load_from_disk` instead.\"\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdownload_mode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDownloadMode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdownload_mode\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mDownloadMode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mREUSE_DATASET_IF_EXISTS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mignore_verifications\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mignore_verifications\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0msave_infos\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;31m# Create a dataset builder\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mbuilder_instance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_dataset_builder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mdata_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mdata_files\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_files\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mcache_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcache_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mfeatures\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mdownload_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdownload_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mdownload_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdownload_mode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mrevision\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrevision\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0muse_auth_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_auth_token\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;34m**\u001b[0m\u001b[0mconfig_kwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;31m# Return iterable dataset in case of streaming\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mif\u001b[0m \u001b[0mstreaming\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mextend_dataset_builder_for_streaming\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuilder_instance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_auth_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_auth_token\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0mbuilder_instance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_streaming_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0msplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m            \u001b[0muse_auth_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_auth_token\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;31m# Some datasets are already processed on the HF google storage\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;31m# Don't try downloading from google storage for the packaged datasets as text, json, csv or pandas\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtry_from_hf_gcs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_PACKAGED_DATASETS_MODULES\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;31m# Download and prepare data\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mbuilder_instance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload_and_prepare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mdownload_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdownload_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mdownload_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdownload_mode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mignore_verifications\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mignore_verifications\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mtry_from_hf_gcs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtry_from_hf_gcs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0muse_auth_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_auth_token\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;31m# Build dataset for splits\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mkeep_in_memory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mkeep_in_memory\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mkeep_in_memory\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mis_small_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuilder_instance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuilder_instance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_verifications\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mignore_verifications\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_memory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeep_in_memory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;31m# Rename and cast features to match task schema\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mif\u001b[0m \u001b[0mtask\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_for_task\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mif\u001b[0m \u001b[0msave_infos\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m        \u001b[0mbuilder_instance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save_infos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;32mreturn\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mFile:\u001b[0m      ~/.local/lib/python3.6/site-packages/datasets/load.py\n",
       "\u001b[0;31mType:\u001b[0m      function\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "load_dataset??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1e2ef8bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-dace524eb0225190\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset csv/default to /home/qblocks/.cache/huggingface/datasets/csv/default-dace524eb0225190/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data files: 100%|██████████| 1/1 [00:00<00:00, 2475.98it/s]\n",
      "Extracting data files: 100%|██████████| 1/1 [00:00<00:00, 451.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to /home/qblocks/.cache/huggingface/datasets/csv/default-dace524eb0225190/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 346.29it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 16000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emtions_from_source = load_dataset(\"csv\", data_files=\"train.txt\", sep=\";\", names=[\"text\", \"label\"])\n",
    "emtions_from_source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "beb54728",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-3923f2a50ece047b\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset csv/default to /home/qblocks/.cache/huggingface/datasets/csv/default-3923f2a50ece047b/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Downloading data: 0.00B [00:00, ?B/s]\u001b[A\n",
      "Downloading data: 641kB [00:00, 4.90MB/s]\u001b[A\n",
      "Downloading data files: 100%|██████████| 1/1 [00:01<00:00,  1.14s/it]\n",
      "Extracting data files: 100%|██████████| 1/1 [00:00<00:00, 502.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to /home/qblocks/.cache/huggingface/datasets/csv/default-3923f2a50ece047b/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 370.59it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 333\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# point the files to the url directly\n",
    "emtions_from_source = load_dataset(\"csv\", data_files=data_url, sep=\";\", names=[\"text\", \"label\"])\n",
    "emtions_from_source"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5a38d7",
   "metadata": {},
   "source": [
    "# From Datasets to DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6111a766",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datasets.dataset_dict.DatasetDict"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emtions.set_format('pandas')\n",
    "type(emtions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dacb9eea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i didnt feel humiliated</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i can go from feeling so hopeless to so damned...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>im grabbing a minute to post i feel greedy wrong</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i am ever feeling nostalgic about the fireplac...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i am feeling grouchy</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0                            i didnt feel humiliated      0\n",
       "1  i can go from feeling so hopeless to so damned...      0\n",
       "2   im grabbing a minute to post i feel greedy wrong      3\n",
       "3  i am ever feeling nostalgic about the fireplac...      2\n",
       "4                               i am feeling grouchy      3"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = emtions['train'][:]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e333b87e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ClassLabel(num_classes=6, names=['sadness', 'joy', 'love', 'anger', 'fear', 'surprise'], id=None)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'fear'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(emtions['train'].features['label'])\n",
    "# lets get the class of label\n",
    "emtions['train'].features['label'].int2str(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3a890e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_int2str(row):\n",
    "    return emtions['train'].features['label'].int2str(row)\n",
    "df['label_name'] = df['label'].apply(label_int2str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0d2444e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>label_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i didnt feel humiliated</td>\n",
       "      <td>0</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i can go from feeling so hopeless to so damned...</td>\n",
       "      <td>0</td>\n",
       "      <td>sadness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>im grabbing a minute to post i feel greedy wrong</td>\n",
       "      <td>3</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i am ever feeling nostalgic about the fireplac...</td>\n",
       "      <td>2</td>\n",
       "      <td>love</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i am feeling grouchy</td>\n",
       "      <td>3</td>\n",
       "      <td>anger</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label label_name\n",
       "0                            i didnt feel humiliated      0    sadness\n",
       "1  i can go from feeling so hopeless to so damned...      0    sadness\n",
       "2   im grabbing a minute to post i feel greedy wrong      3      anger\n",
       "3  i am ever feeling nostalgic about the fireplac...      2       love\n",
       "4                               i am feeling grouchy      3      anger"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "051b0e2e",
   "metadata": {},
   "source": [
    "## Looking at the Class Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2ff50769",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "02ce967d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df['label_name'].value_counts(ascending=True).plot.barh()\n",
    "plt.title(\"Frequency of Classes\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6cc83006",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>label_name</th>\n",
       "      <th>words_per_tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i didnt feel humiliated</td>\n",
       "      <td>0</td>\n",
       "      <td>sadness</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i can go from feeling so hopeless to so damned...</td>\n",
       "      <td>0</td>\n",
       "      <td>sadness</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>im grabbing a minute to post i feel greedy wrong</td>\n",
       "      <td>3</td>\n",
       "      <td>anger</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i am ever feeling nostalgic about the fireplac...</td>\n",
       "      <td>2</td>\n",
       "      <td>love</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i am feeling grouchy</td>\n",
       "      <td>3</td>\n",
       "      <td>anger</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label label_name  \\\n",
       "0                            i didnt feel humiliated      0    sadness   \n",
       "1  i can go from feeling so hopeless to so damned...      0    sadness   \n",
       "2   im grabbing a minute to post i feel greedy wrong      3      anger   \n",
       "3  i am ever feeling nostalgic about the fireplac...      2       love   \n",
       "4                               i am feeling grouchy      3      anger   \n",
       "\n",
       "   words_per_tweet  \n",
       "0                4  \n",
       "1               21  \n",
       "2               10  \n",
       "3               18  \n",
       "4                4  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['words_per_tweet'] = df['text'].str.split().apply(len)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e131b3af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.boxplot(\"words_per_tweet\", by=\"label_name\", grid=False,\n",
    "          showfliers=False, color=\"black\")\n",
    "plt.suptitle(\"\")\n",
    "plt.xlabel(\"\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3362e0db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>label_name</th>\n",
       "      <th>words_per_tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i didnt feel humiliated</td>\n",
       "      <td>0</td>\n",
       "      <td>sadness</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i can go from feeling so hopeless to so damned...</td>\n",
       "      <td>0</td>\n",
       "      <td>sadness</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>im grabbing a minute to post i feel greedy wrong</td>\n",
       "      <td>3</td>\n",
       "      <td>anger</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i am ever feeling nostalgic about the fireplac...</td>\n",
       "      <td>2</td>\n",
       "      <td>love</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i am feeling grouchy</td>\n",
       "      <td>3</td>\n",
       "      <td>anger</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label label_name  \\\n",
       "0                            i didnt feel humiliated      0    sadness   \n",
       "1  i can go from feeling so hopeless to so damned...      0    sadness   \n",
       "2   im grabbing a minute to post i feel greedy wrong      3      anger   \n",
       "3  i am ever feeling nostalgic about the fireplac...      2       love   \n",
       "4                               i am feeling grouchy      3      anger   \n",
       "\n",
       "   words_per_tweet  \n",
       "0                4  \n",
       "1               21  \n",
       "2               10  \n",
       "3               18  \n",
       "4                4  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emtions.reset_format()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b4bdc6de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 16000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emtions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf70b755",
   "metadata": {},
   "source": [
    "# From Text to Tokens\n",
    "Transformers models are expected to receive numerical vector that represent the whole text, and for that we first tokenize that text into tokens, then from these tokens get the numerical representation:\n",
    "\n",
    "Their are different tokenization strategies either:\n",
    "\n",
    "- Character Tokenization, represent string as list of chars\n",
    "- word Tokenization, represent string as list of words\n",
    "- \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cd57a70d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['T', 'o', 'k', 'e', 'n', 'i', 'z', 'i', 'n', 'g', ' ', 't', 'e', 'x', 't', ' ', 'i', 's', ' ', 'a', ' ', 'c', 'o', 'r', 'e', ' ', 't', 'a', 's', 'k', ' ', 'o', 'f', ' ', 'N', 'L', 'P', '.']\n",
      "==================================================\n",
      "{' ': 0, '.': 1, 'L': 2, 'N': 3, 'P': 4, 'T': 5, 'a': 6, 'c': 7, 'e': 8, 'f': 9, 'g': 10, 'i': 11, 'k': 12, 'n': 13, 'o': 14, 'r': 15, 's': 16, 't': 17, 'x': 18, 'z': 19}\n",
      "==================================================\n",
      "[5, 14, 12, 8, 13, 11, 19, 11, 13, 10, 0, 17, 8, 18, 17, 0, 11, 16, 0, 6, 0, 7, 14, 15, 8, 0, 17, 6, 16, 12, 0, 14, 9, 0, 3, 2, 4, 1]\n"
     ]
    }
   ],
   "source": [
    "# Character Tokenization\n",
    "text = \"Tokenizing text is a core task of NLP.\"\n",
    "tokenized_text = list(text)\n",
    "print(tokenized_text)\n",
    "\n",
    "# Map chars into numbers\n",
    "print(\"=\"*50)\n",
    "token2idx = {ch: val for val, ch in enumerate(sorted(set(tokenized_text)))}\n",
    "print(token2idx)\n",
    "\n",
    "# Encode chars into numbers\n",
    "print(\"=\"*50)\n",
    "input_ids = [token2idx[token] for token in tokenized_text]\n",
    "print(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "967b6fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d15bf643",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 5, 14, 12,  8, 13, 11, 19, 11, 13, 10,  0, 17,  8, 18, 17,  0, 11, 16,\n",
       "         0,  6,  0,  7, 14, 15,  8,  0, 17,  6, 16, 12,  0, 14,  9,  0,  3,  2,\n",
       "         4,  1])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids = F.torch.tensor(input_ids)\n",
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d0a938ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([38, 20])\n",
      "==================================================\n",
      "38\n",
      "==================================================\n",
      "20\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert the input ids into one hot matrix\n",
    "one_hot_encodings = F.one_hot(input_ids, num_classes=len(token2idx))\n",
    "print(one_hot_encodings.shape)\n",
    "print(\"=\"*50)\n",
    "print(len(input_ids))\n",
    "print(\"=\"*50)\n",
    "print(len(token2idx))\n",
    "# For each of the 38 input tokens we now have a one-hot vector with 20 dimensions\n",
    "# this operation help avoid consider input_ids as numbers like 5, 14 to detected as pattern\n",
    "one_hot_encodings[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d3bc36a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: T\n",
      "Tensor index: 5\n",
      "One-hot: tensor([0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "print(f\"Token: {tokenized_text[0]}\")\n",
    "print(f\"Tensor index: {input_ids[0]}\")\n",
    "# we can see the index of 5 is 1\n",
    "print(f\"One-hot: {one_hot_encodings[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c7e18fe",
   "metadata": {},
   "source": [
    "# Word Tokenization\n",
    "\n",
    "One of the ways to tokenize text is to get the words of that text into list.\n",
    "But the language are free grammer, free of errors and so on, so we can expect large number of words, may be millions and even more in large dataset.\n",
    "But the challange is that when we tend to compress these millions of different words into 300 dimension, and this cause the netwrok to have layers with billions of parameters !\n",
    "\n",
    "So instead of lossing some information by ignoring some words and take only first 10000 common words or larger of smaller than that numbers, we can use another way which is \"Subword Tokenization\", using this approach we can have all the information and do not mission some words.\n",
    "\n",
    "# Subword Tokenization\n",
    "The basic idea behind subword tokenization is to combine the best aspects of character and word tokenization.\n",
    "\n",
    "The main distinguishing feature of subword tokenization (as well as word tokenization) is that it is learned from the pretraining corpus using a mix of statistical rules and algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "af887885",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5c76c6b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 28.0/28.0 [00:00<00:00, 10.6kB/s]\n",
      "Downloading: 100%|██████████| 483/483 [00:00<00:00, 197kB/s]\n",
      "Downloading: 100%|██████████| 226k/226k [00:00<00:00, 1.04MB/s]\n",
      "Downloading: 100%|██████████| 455k/455k [00:00<00:00, 1.66MB/s]\n"
     ]
    }
   ],
   "source": [
    "model_ckpt = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "90c809a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [101, 19204, 6026, 3793, 2003, 1037, 4563, 4708, 1997, 17953, 2361, 1012, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "encoded_text = tokenizer(text)\n",
    "print(encoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "801a2bf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'token', '##izing', 'text', 'is', 'a', 'core', 'task', 'of', 'nl', '##p', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer.convert_ids_to_tokens(encoded_text.input_ids)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f20f9c",
   "metadata": {},
   "source": [
    "# Note !\n",
    "\n",
    "- [CLS] and [SEP], these two tokens define the start and end of the sequence.\n",
    "- Tokens are in lower case, this ensure that there are some preprocessing in that tokenizer\n",
    "- Since NLP and Tokenization are not language words, so it tokenized to sub-tokens, and ## means that the preceding string is not whitespace.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "75431007",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] tokenizing text is a core task of nlp. [SEP]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.convert_tokens_to_string(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2fde1ccd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30522\n",
      "512\n",
      "['input_ids', 'attention_mask']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.vocab_size)\n",
    "print(tokenizer.model_max_length)\n",
    "\n",
    "# this is means the model except the input_ids and attention_mask\n",
    "print(tokenizer.model_input_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c33a3617",
   "metadata": {},
   "source": [
    "# Tokenizing the Whole Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "66469259",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(batch):\n",
    "    return tokenizer(batch['text'], padding=True, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3381cf85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[101, 1045, 2134, 2102, 2514, 26608, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 1045, 2064, 2175, 2013, 3110, 2061, 20625, 2000, 2061, 9636, 17772, 2074, 2013, 2108, 2105, 2619, 2040, 14977, 1998, 2003, 8300, 102]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n"
     ]
    }
   ],
   "source": [
    "print(tokenize(emtions[\"train\"][:2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f3d1f1",
   "metadata": {},
   "source": [
    "# Notes !\n",
    "\n",
    "in input ids:\n",
    "\n",
    "- 100 for UNK (unkown words)\n",
    "- 101 = [CLS] token\n",
    "- 102 = [SEP] token\n",
    "- 103 = masked words\n",
    "- 0 for padding\n",
    "\n",
    "in attention_mask:\n",
    "- 0 for absence words, in meet of 0 in input ids\n",
    "- 1 for existing words\n",
    "\n",
    "The attention mask helo the model to ignore the padding of the input_ids, so it have 0 in meet of these 0 in input ids.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "abccd7ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.80s/ba]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.58ba/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.27ba/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 16000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# batch_size =None, will process the different parts of dataset as one batch like train, test, validation\n",
    "emtions_encoded = emtions.map(tokenize, batched=True, batch_size=None)\n",
    "emtions_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1f2d426e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 16000\n",
       "})"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emtions_encoded['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "521d1b13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label'],\n",
       "    num_rows: 16000\n",
       "})"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emtions['train']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123340db",
   "metadata": {},
   "source": [
    "# Note !\n",
    "\n",
    "The map function added two new columns which the input_ids and attention_mask related to each tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00925c72",
   "metadata": {},
   "source": [
    "# Training a Text Classifier\n",
    "\n",
    "we need to combine pretrained body with a custom classification head, because most of these models are language models, that trying to predict the masked word.\n",
    "\n",
    "## Two ways !\n",
    "\n",
    "There are two ways of using these language models, either by using the hidden states tha represent the text as it, and use it for features engineering or fine tune these states via training the model end to end with our dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ac80468d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2fa6b1df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 256M/256M [00:06<00:00, 43.9MB/s] \n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_projector.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else  \"cpu\")\n",
    "print(device)\n",
    "model = AutoModel.from_pretrained(model_ckpt).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d5d358a",
   "metadata": {},
   "source": [
    "# Note !\n",
    "This is torch model but if we need to change to tensorflow, just put TF before AutoModel.\n",
    "so either model in torch or tensorflow we can easily convery between them.\n",
    "\n",
    "But we need to change the paramter from_pt=True, when call TFAutoModel.from_pretrained(), the vise verse change \n",
    "from_tf=True when call AutoModel.from_pretrained(), this is in case we need to load different model in other framework like load toch model into tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607da918",
   "metadata": {},
   "source": [
    "# Extracting the last hidden states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9b3cd57e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 101, 2023, 2003, 1037, 3231,  102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1]])}\n",
      "torch.Size([1, 6])\n"
     ]
    }
   ],
   "source": [
    "text = \"this is a test\"\n",
    "# directly return the tensors in torch\n",
    "inputs = tokenizer(text, return_tensors='pt')\n",
    "print(inputs)\n",
    "print(inputs.input_ids.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "85bfee3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 101, 2023, 2003, 1037, 3231,  102]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1]], device='cuda:0')}\n"
     ]
    }
   ],
   "source": [
    "# load to gpu if available\n",
    "inputs = {k:v.to(device) for k, v in inputs.items()}\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "df8a705b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['input_ids', 'attention_mask']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To know what the model need\n",
    "tokenizer.model_input_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b3ca7963",
   "metadata": {},
   "outputs": [],
   "source": [
    "# no_grad disable the automatic graident calculation\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2295518a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BaseModelOutput(last_hidden_state=tensor([[[-0.1564, -0.1862,  0.0527,  ..., -0.1188,  0.0663,  0.5469],\n",
      "         [-0.3570, -0.6484, -0.0618,  ..., -0.3037,  0.3507,  0.5219],\n",
      "         [-0.2767, -0.4461,  0.1818,  ..., -0.0948, -0.0078,  0.9956],\n",
      "         [-0.2835, -0.3919,  0.3749,  ..., -0.2149, -0.1170,  1.0524],\n",
      "         [ 0.2663, -0.5093, -0.3182,  ..., -0.4204,  0.0144, -0.2151],\n",
      "         [ 0.9444,  0.0111, -0.4720,  ...,  0.1440, -0.7283, -0.1619]]],\n",
      "       device='cuda:0'), hidden_states=None, attentions=None)\n"
     ]
    }
   ],
   "source": [
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "40a7a877",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 6, 768])\n"
     ]
    }
   ],
   "source": [
    "# The 6 tokens encoding above converted to 1 for batch size * 6 * 768 for each token which is the token embedding !\n",
    "print(outputs['last_hidden_state'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c193392",
   "metadata": {},
   "source": [
    "# Using first token \n",
    "\n",
    "For classification tasks, it is common practice to just use the hidden state associated with the [CLS] token as the input feature. Since this token appears at the start of each sequence, we can extract it by simply indexing into outputs.last_hidden_state as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9ba2792a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.last_hidden_state[:, 0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8986cfce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_hidden_states(batch):\n",
    "    # Place model inputs on the GPU\n",
    "    inputs = {k: v.to(device) for k, v in batch.items() if k in tokenizer.model_input_names}\n",
    "    # Extract last hidden states\n",
    "    with torch.no_grad():\n",
    "        last_hidden_state = model(**inputs).last_hidden_state[:, 0]\n",
    "    # cpu().numpy() because the map method except python or numpy object\n",
    "    return {\"last_hidden_state\": last_hidden_state.cpu().numpy()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a7f72422",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['text', 'label', 'input_ids', 'attention_mask']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 16000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 2000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# because the model except tensors\n",
    "emtions_encoded.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "print(emtions_encoded['train'].column_names)\n",
    "emtions_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "af926242",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [00:08<00:00, 28.72ba/s]\n",
      "100%|██████████| 32/32 [00:00<00:00, 36.71ba/s]\n",
      "100%|██████████| 32/32 [00:00<00:00, 38.07ba/s]\n"
     ]
    }
   ],
   "source": [
    "emotions_hidden = emtions_encoded.map(extract_hidden_states, batched=True, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7434f9ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['text', 'label', 'input_ids', 'attention_mask']\n"
     ]
    }
   ],
   "source": [
    "print(emtions_encoded['train'].column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3dbcd81",
   "metadata": {},
   "source": [
    "# Note !\n",
    "Now that we have the hidden states associated with each tweet, the next step is to train a classifier on them. To do that, we’ll need a feature matrix—let’s take a look."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57ebd59",
   "metadata": {},
   "source": [
    "# Creating a feature matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "6df64e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "bae24202",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16000, 768) (2000, 768)\n"
     ]
    }
   ],
   "source": [
    "X_train = np.array(emotions_hidden[\"train\"][\"last_hidden_state\"])\n",
    "X_valid = np.array(emotions_hidden[\"validation\"][\"last_hidden_state\"])\n",
    "y_train = np.array(emotions_hidden[\"train\"][\"label\"])\n",
    "y_valid = np.array(emotions_hidden[\"validation\"][\"label\"])\n",
    "print(X_train.shape, X_valid.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e481c64",
   "metadata": {},
   "source": [
    "# Visualizing the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ffa83535-2164-4f86-80e3-ba399f949c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# it takes time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3fb40095",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from umap import UMAP\n",
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# # Scale features to [0,1] range\n",
    "# X_scaled = MinMaxScaler().fit_transform(X_train)\n",
    "# # Initialize and fit UMAP\n",
    "# mapper = UMAP(n_components=2, metric=\"cosine\").fit(X_scaled)\n",
    "# # Create a DataFrame of 2D embeddings\n",
    "# df_emb = pd.DataFrame(mapper.embedding_, columns=[\"X\", \"Y\"])\n",
    "# df_emb[\"label\"] = y_train\n",
    "# df_emb.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e11abaa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, axes = plt.subplots(2, 3, figsize=(7,5))\n",
    "# axes = axes.flatten()\n",
    "# cmaps = [\"Greys\", \"Blues\", \"Oranges\", \"Reds\", \"Purples\", \"Greens\"]\n",
    "# labels = emotions[\"train\"].features[\"label\"].names\n",
    "\n",
    "# for i, (label, cmap) in enumerate(zip(labels, cmaps)):\n",
    "#     df_emb_sub = df_emb.query(f\"label == {i}\")\n",
    "#     axes[i].hexbin(df_emb_sub[\"X\"], df_emb_sub[\"Y\"], cmap=cmap,\n",
    "#                    gridsize=20, linewidths=(0,))\n",
    "#     axes[i].set_title(label)\n",
    "#     axes[i].set_xticks([]), axes[i].set_yticks([])\n",
    "    \n",
    "# plt.tight_layobut()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6cb754d",
   "metadata": {},
   "source": [
    "# Note !\n",
    "\n",
    "From this plot we can see some clear patterns: the negative feelings such as sadness, anger, and fear all occupy similar regions with slightly varying distributions. On the other hand, joy and love are well separated from the negative emotions and also share a similar space.\n",
    "\n",
    "We may have hoped for some separation, this is in no way guaranteed since the model was not trained to know the difference between these emotions. It only learned them implicitly by guessing the masked words in texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "916744e6-a94d-42d0-9bab-86825bcc5303",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "9db37fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c60cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We increase `max_iter` to guarantee convergence\n",
    "lr_clf = LogisticRegression(max_iter=100)\n",
    "lr_clf.fit(X_train, y_train)\n",
    "lr_clf.score(X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c482d5bd-be1e-4741-a1b3-bc26f39207ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = emotions[\"train\"].features[\"label\"].names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c975355",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_preds, y_true, labels):\n",
    "    cm = confusion_matrix(y_true, y_preds, normalize=\"true\")\n",
    "    fig, ax = plt.subplots(figsize=(6, 6))\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
    "    disp.plot(cmap=\"Blues\", values_format=\".2f\", ax=ax, colorbar=False)\n",
    "    plt.title(\"Normalized confusion matrix\")\n",
    "    plt.show()\n",
    "\n",
    "y_preds = lr_clf.predict(X_valid)\n",
    "plot_confusion_matrix(y_preds, y_valid, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb7f2e88",
   "metadata": {},
   "source": [
    "# Fine-Tuning Transformers\n",
    "\n",
    "Instead of using the AutoModel we will use another one that has the head layer of classifcation and pass to it the number of classes we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b57206b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f2dd67",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_labels = 6\n",
    "\n",
    "model = (AutoModelForSequenceClassification.from_pretrained(model_ckpt, num_labels=num_labels).to(device()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972e8e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed041c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    f1 = f1_score(labels, preds, average=\"weighted\")\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\"accuracy\": acc, \"f1\": f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e0c1b3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Login successful\n",
      "Your token has been saved to /home/abdelrahman/.huggingface/token\n",
      "\u001b[1m\u001b[31mAuthenticated through git-credential store but this isn't the helper defined on your machine.\n",
      "You might have to re-authenticate when pushing to the Hugging Face Hub. Run the following command in your terminal in case you want to set this credential helper as the default\n",
      "\n",
      "git config --global credential.helper store\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e1bfe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35b6237",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18393130",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "logging_steps = len(emtions_encoded[\"train\"]) // batch_size\n",
    "\n",
    "model_name = f\"{model_ckpt}-finetuned-emotion\"\n",
    "\n",
    "training_args = TrainingArguments(output_dir=model_name,\n",
    "                                  num_train_epochs=2,\n",
    "                                  learning_rate=2e-5,\n",
    "                                  per_device_train_batch_size=batch_size,\n",
    "                                  per_device_eval_batch_size=batch_size,\n",
    "                                  weight_decay=0.01,\n",
    "                                  evaluation_strategy=\"epoch\",\n",
    "                                  disable_tqdm=False,\n",
    "                                  logging_steps=logging_steps,\n",
    "                                  push_to_hub=True,\n",
    "                                  log_level=\"error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769b24af",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model=model, args=training_args,\n",
    "                  compute_metrics=compute_metrics,\n",
    "                  train_dataset=emotions_encoded[\"train\"],\n",
    "                  eval_dataset=emotions_encoded[\"validation\"],\n",
    "                  tokenizer=tokenizer)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ceba1fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_output = trainer.predict(emotions_encoded[\"validation\"])\n",
    "preds_output.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1837a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds = np.argmax(preds_output.predictions, axis=1)\n",
    "plot_confusion_matrix(y_preds, y_valid, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be47f8ea",
   "metadata": {},
   "source": [
    "# Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449198e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.functional import cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ac9acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass_with_label(batch):\n",
    "    # Place all input tensors on the same device as the model\n",
    "    inputs = {k:v.to(device) for k,v in batch.items()\n",
    "              if k in tokenizer.model_input_names}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(**inputs)\n",
    "        pred_label = torch.argmax(output.logits, axis=-1)\n",
    "        loss = cross_entropy(output.logits, batch[\"label\"].to(device),\n",
    "                             reduction=\"none\")\n",
    "    # Place outputs on CPU for compatibility with other dataset columns\n",
    "    return {\"loss\": loss.cpu().numpy(),\n",
    "            \"predicted_label\": pred_label.cpu().numpy()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b875af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert our dataset back to PyTorch tensors\n",
    "emotions_encoded.set_format(\"torch\",\n",
    "                            columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "# Compute loss values\n",
    "emotions_encoded[\"validation\"] = emotions_encoded[\"validation\"].map(\n",
    "    forward_pass_with_label, batched=True, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336c108d",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions_encoded.set_format(\"pandas\")\n",
    "cols = [\"text\", \"label\", \"predicted_label\", \"loss\"]\n",
    "df_test = emotions_encoded[\"validation\"][:][cols]\n",
    "df_test[\"label\"] = df_test[\"label\"].apply(label_int2str)\n",
    "df_test[\"predicted_label\"] = (df_test[\"predicted_label\"]\n",
    "                              .apply(label_int2str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b26fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.sort_values(\"loss\", ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a4d0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.sort_values(\"loss\", ascending=True).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b05a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.push_to_hub(commit_message=\"Training completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf2b844",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b91bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"A`bdelrahman-Rezk/distilbert-base-uncased-finetuned-emotion\"\n",
    "classifier = pipeline(\"text-classification\", model=model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80810e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_tweet = \"I saw a movie today and it was really good.\"\n",
    "preds = classifier(custom_tweet, return_all_scores=True)\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77e07aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_df = pd.DataFrame(preds[0])\n",
    "plt.bar(labels, 100 * preds_df[\"score\"], color='C0')\n",
    "plt.title(f'\"{custom_tweet}\"')\n",
    "plt.ylabel(\"Class probability (%)\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
