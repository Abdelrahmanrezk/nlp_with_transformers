{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6560ce1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install transformers\n",
    "# !pip3 install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89dec069",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, list_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd43b634",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(list_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6cb751b",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_datasets = list_datasets()\n",
    "print(\"The number of Avaliable datasets and it can be increased as based on datasets version: \", len(all_datasets))\n",
    "all_datasets[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c35e354",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets use emotion dataset\n",
    "# it takes a time to load the dataset for first time, then it use the cassed version next time you run the code\n",
    "emtions = load_dataset('emotion')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d896f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(emtions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4b5154",
   "metadata": {},
   "outputs": [],
   "source": [
    "emtions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ddcd58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# looks like dictionary So !\n",
    "train_data = emtions['train']\n",
    "print(type(train_data))\n",
    "train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1388a35e",
   "metadata": {},
   "source": [
    "# Dataset Object\n",
    "\n",
    "This class of **Dataset** one of the strong data structure in the dataset library, its like python list, so we can access using indexed way. \n",
    "This **Dataset** data structure based on Appache arrow which defines type of column format than more memory effient than python code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34b774b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(train_data[:3]))\n",
    "\n",
    "train_data[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda3c93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if we just interested in just the labels\n",
    "train_data['label'][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e0cd41",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.column_names # as like data frame get the different columns in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04ebcf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As saw above this Dataset object has features key and it hold information about each col datatype\n",
    "train_data.features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10880fdc",
   "metadata": {},
   "source": [
    "# load_dataset\n",
    "\n",
    "Its not only about the dataset available on Huggin hub, its about other data either on your machine of anywhere, it also can handle different types of the dataset like csv and other.\n",
    "\n",
    "The load_dataset use the corresponding  script file for your datafile to load it.\n",
    "\n",
    "**Lets get emotion dataset from its source**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7c2fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_url = \"https://www.dropbox.com/s/1pzkadrvffbqw6o/train.txt\"\n",
    "!wget {data_url}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93ee5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd train.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ebcb7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -5 train.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab43fed",
   "metadata": {},
   "source": [
    "# lets load that file into dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf14eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dataset??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2ef8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "emtions_from_source = load_dataset(\"csv\", data_files=\"train.txt\", sep=\";\", names=[\"text\", \"label\"])\n",
    "emtions_from_source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb54728",
   "metadata": {},
   "outputs": [],
   "source": [
    "# point the files to the url directly\n",
    "emtions_from_source = load_dataset(\"csv\", data_files=data_url, sep=\";\", names=[\"text\", \"label\"])\n",
    "emtions_from_source"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5a38d7",
   "metadata": {},
   "source": [
    "# From Datasets to DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6111a766",
   "metadata": {},
   "outputs": [],
   "source": [
    "emtions.set_format('pandas')\n",
    "type(emtions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dacb9eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = emtions['train'][:]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e333b87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(emtions['train'].features['label'])\n",
    "# lets get the class of label\n",
    "emtions['train'].features['label'].int2str(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a890e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_int2str(row):\n",
    "    return emtions['train'].features['label'].int2str(row)\n",
    "df['label_name'] = df['label'].apply(label_int2str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2444e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "051b0e2e",
   "metadata": {},
   "source": [
    "## Looking at the Class Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff50769",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ce967d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['label_name'].value_counts(ascending=True).plot.barh()\n",
    "plt.title(\"Frequency of Classes\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc83006",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['words_per_tweet'] = df['text'].str.split().apply(len)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e131b3af",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.boxplot(\"words_per_tweet\", by=\"label_name\", grid=False,\n",
    "          showfliers=False, color=\"black\")\n",
    "plt.suptitle(\"\")\n",
    "plt.xlabel(\"\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3362e0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "emtions.reset_format()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4bdc6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "emtions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf70b755",
   "metadata": {},
   "source": [
    "# From Text to Tokens\n",
    "Transformers models are expected to receive numerical vector that represent the whole text, and for that we first tokenize that text into tokens, then from these tokens get the numerical representation:\n",
    "\n",
    "Their are different tokenization strategies either:\n",
    "\n",
    "- Character Tokenization, represent string as list of chars\n",
    "- word Tokenization, represent string as list of words\n",
    "- \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd57a70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Character Tokenization\n",
    "text = \"Tokenizing text is a core task of NLP.\"\n",
    "tokenized_text = list(text)\n",
    "print(tokenized_text)\n",
    "\n",
    "# Map chars into numbers\n",
    "print(\"=\"*50)\n",
    "token2idx = {ch: val for val, ch in enumerate(sorted(set(tokenized_text)))}\n",
    "print(token2idx)\n",
    "\n",
    "# Encode chars into numbers\n",
    "print(\"=\"*50)\n",
    "input_ids = [token2idx[token] for token in tokenized_text]\n",
    "print(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967b6fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15bf643",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = F.torch.tensor(input_ids)\n",
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a938ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the input ids into one hot matrix\n",
    "one_hot_encodings = F.one_hot(input_ids, num_classes=len(token2idx))\n",
    "print(one_hot_encodings.shape)\n",
    "print(\"=\"*50)\n",
    "print(len(input_ids))\n",
    "print(\"=\"*50)\n",
    "print(len(token2idx))\n",
    "# For each of the 38 input tokens we now have a one-hot vector with 20 dimensions\n",
    "# this operation help avoid consider input_ids as numbers like 5, 14 to detected as pattern\n",
    "one_hot_encodings[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3bc36a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Token: {tokenized_text[0]}\")\n",
    "print(f\"Tensor index: {input_ids[0]}\")\n",
    "# we can see the index of 5 is 1\n",
    "print(f\"One-hot: {one_hot_encodings[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c7e18fe",
   "metadata": {},
   "source": [
    "# Word Tokenization\n",
    "\n",
    "One of the ways to tokenize text is to get the words of that text into list.\n",
    "But the language are free grammer, free of errors and so on, so we can expect large number of words, may be millions and even more in large dataset.\n",
    "But the challange is that when we tend to compress these millions of different words into 300 dimension, and this cause the netwrok to have layers with billions of parameters !\n",
    "\n",
    "So instead of lossing some information by ignoring some words and take only first 10000 common words or larger of smaller than that numbers, we can use another way which is \"Subword Tokenization\", using this approach we can have all the information and do not mission some words.\n",
    "\n",
    "# Subword Tokenization\n",
    "The basic idea behind subword tokenization is to combine the best aspects of character and word tokenization.\n",
    "\n",
    "The main distinguishing feature of subword tokenization (as well as word tokenization) is that it is learned from the pretraining corpus using a mix of statistical rules and algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af887885",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c76c6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ckpt = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c809a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_text = tokenizer(text)\n",
    "print(encoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801a2bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer.convert_ids_to_tokens(encoded_text.input_ids)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f20f9c",
   "metadata": {},
   "source": [
    "# Note !\n",
    "\n",
    "- [CLS] and [SEP], these two tokens define the start and end of the sequence.\n",
    "- Tokens are in lower case, this ensure that there are some preprocessing in that tokenizer\n",
    "- Since NLP and Tokenization are not language words, so it tokenized to sub-tokens, and ## means that the preceding string is not whitespace.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75431007",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.convert_tokens_to_string(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fde1ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.vocab_size)\n",
    "print(tokenizer.model_max_length)\n",
    "\n",
    "# this is means the model except the input_ids and attention_mask\n",
    "print(tokenizer.model_input_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c33a3617",
   "metadata": {},
   "source": [
    "# Tokenizing the Whole Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66469259",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(batch):\n",
    "    return tokenizer(batch['text'], padding=True, truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3381cf85",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenize(emtions[\"train\"][:2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f3d1f1",
   "metadata": {},
   "source": [
    "# Notes !\n",
    "\n",
    "in input ids:\n",
    "\n",
    "- 100 for UNK (unkown words)\n",
    "- 101 = [CLS] token\n",
    "- 102 = [SEP] token\n",
    "- 103 = masked words\n",
    "- 0 for padding\n",
    "\n",
    "in attention_mask:\n",
    "- 0 for absence words, in meet of 0 in input ids\n",
    "- 1 for existing words\n",
    "\n",
    "The attention mask helo the model to ignore the padding of the input_ids, so it have 0 in meet of these 0 in input ids.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abccd7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size =None, will process the different parts of dataset as one batch like train, test, validation\n",
    "emtions_encoded = emtions.map(tokenize, batched=True, batch_size=None)\n",
    "emtions_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f2d426e",
   "metadata": {},
   "outputs": [],
   "source": [
    "emtions_encoded['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521d1b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "emtions['train']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123340db",
   "metadata": {},
   "source": [
    "# Note !\n",
    "\n",
    "The map function added two new columns which the input_ids and attention_mask related to each tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00925c72",
   "metadata": {},
   "source": [
    "# Training a Text Classifier\n",
    "\n",
    "we need to combine pretrained body with a custom classification head, because most of these models are language models, that trying to predict the masked word.\n",
    "\n",
    "## Two ways !\n",
    "\n",
    "There are two ways of using these language models, either by using the hidden states tha represent the text as it, and use it for features engineering or fine tune these states via training the model end to end with our dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac80468d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa6b1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else  \"cpu\")\n",
    "print(device)\n",
    "model = AutoModel.from_pretrained(model_ckpt).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d5d358a",
   "metadata": {},
   "source": [
    "# Note !\n",
    "This is torch model but if we need to change to tensorflow, just put TF before AutoModel.\n",
    "so either model in torch or tensorflow we can easily convery between them.\n",
    "\n",
    "But we need to change the paramter from_pt=True, when call TFAutoModel.from_pretrained(), the vise verse change \n",
    "from_tf=True when call AutoModel.from_pretrained(), this is in case we need to load different model in other framework like load toch model into tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5331164",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFAutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53da9721",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_tf = TFAutoModel.from_pretrained(model_ckpt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607da918",
   "metadata": {},
   "source": [
    "# Extracting the last hidden states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3cd57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"this is a test\"\n",
    "# directly return the tensors in torch\n",
    "inputs = tokenizer(text, return_tensors='pt')\n",
    "print(inputs)\n",
    "print(inputs.input_ids.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85bfee3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load to gpu if available\n",
    "inputs = {k:v.to(device) for k, v in inputs.items()}\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8a705b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To know what the model need\n",
    "tokenizer.model_input_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ca7963",
   "metadata": {},
   "outputs": [],
   "source": [
    "# no_grad disable the automatic graident calculation\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2295518a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a7a877",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The 6 tokens encoding above converted to 1 for batch size * 6 * 768 for each token which is the token embedding !\n",
    "print(outputs['last_hidden_state'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c193392",
   "metadata": {},
   "source": [
    "# Using first token \n",
    "\n",
    "For classification tasks, it is common practice to just use the hidden state associated with the [CLS] token as the input feature. Since this token appears at the start of each sequence, we can extract it by simply indexing into outputs.last_hidden_state as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba2792a",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs.last_hidden_state[:, 0].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8986cfce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_hidden_states(batch):\n",
    "    # Place model inputs on the GPU\n",
    "    inputs = {k: v.to(device) for k, v in batch.items() if k in tokenizer.model_input_names}\n",
    "    # Extract last hidden states\n",
    "    with torch.no_grad():\n",
    "        last_hidden_state = model(**inputs).last_hidden_state[:, 0]\n",
    "    # cpu().numpy() because the map method except python or numpy object\n",
    "    return {\"last_hidden_state\": last_hidden_state.cpu().numpy()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f72422",
   "metadata": {},
   "outputs": [],
   "source": [
    "# because the model except tensors\n",
    "emtions_encoded.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n",
    "print(emtions_encoded['train'].column_names)\n",
    "emtions_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af926242",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions_hidden = emtions_encoded.map(extract_hidden_states, batched=True, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7434f9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(emtions_encoded['train'].column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3dbcd81",
   "metadata": {},
   "source": [
    "# Note !\n",
    "Now that we have the hidden states associated with each tweet, the next step is to train a classifier on them. To do that, we’ll need a feature matrix—let’s take a look."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57ebd59",
   "metadata": {},
   "source": [
    "# Creating a feature matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df64e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae24202",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array(emotions_hidden[\"train\"][\"last_hidden_state\"])\n",
    "X_valid = np.array(emotions_hidden[\"validation\"][\"last_hidden_state\"])\n",
    "y_train = np.array(emotions_hidden[\"train\"][\"label\"])\n",
    "y_valid = np.array(emotions_hidden[\"validation\"][\"label\"])\n",
    "print(X_train.shape, X_valid.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e481c64",
   "metadata": {},
   "source": [
    "# Visualizing the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb40095",
   "metadata": {},
   "outputs": [],
   "source": [
    "from umap import UMAP\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Scale features to [0,1] range\n",
    "X_scaled = MinMaxScaler().fit_transform(X_train)\n",
    "# Initialize and fit UMAP\n",
    "mapper = UMAP(n_components=2, metric=\"cosine\").fit(X_scaled)\n",
    "# Create a DataFrame of 2D embeddings\n",
    "df_emb = pd.DataFrame(mapper.embedding_, columns=[\"X\", \"Y\"])\n",
    "df_emb[\"label\"] = y_train\n",
    "df_emb.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11abaa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 3, figsize=(7,5))\n",
    "axes = axes.flatten()\n",
    "cmaps = [\"Greys\", \"Blues\", \"Oranges\", \"Reds\", \"Purples\", \"Greens\"]\n",
    "labels = emotions[\"train\"].features[\"label\"].names\n",
    "\n",
    "for i, (label, cmap) in enumerate(zip(labels, cmaps)):\n",
    "    df_emb_sub = df_emb.query(f\"label == {i}\")\n",
    "    axes[i].hexbin(df_emb_sub[\"X\"], df_emb_sub[\"Y\"], cmap=cmap,\n",
    "                   gridsize=20, linewidths=(0,))\n",
    "    axes[i].set_title(label)\n",
    "    axes[i].set_xticks([]), axes[i].set_yticks([])\n",
    "    \n",
    "plt.tight_layobut()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6cb754d",
   "metadata": {},
   "source": [
    "# Note !\n",
    "\n",
    "From this plot we can see some clear patterns: the negative feelings such as sadness, anger, and fear all occupy similar regions with slightly varying distributions. On the other hand, joy and love are well separated from the negative emotions and also share a similar space.\n",
    "\n",
    "We may have hoped for some separation, this is in no way guaranteed since the model was not trained to know the difference between these emotions. It only learned them implicitly by guessing the masked words in texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db37fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c60cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We increase `max_iter` to guarantee convergence\n",
    "lr_clf = LogisticRegression(max_iter=3000)\n",
    "lr_clf.fit(X_train, y_train)\n",
    "lr_clf.score(X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c975355",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_preds, y_true, labels):\n",
    "    cm = confusion_matrix(y_true, y_preds, normalize=\"true\")\n",
    "    fig, ax = plt.subplots(figsize=(6, 6))\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
    "    disp.plot(cmap=\"Blues\", values_format=\".2f\", ax=ax, colorbar=False)\n",
    "    plt.title(\"Normalized confusion matrix\")\n",
    "    plt.show()\n",
    "\n",
    "y_preds = lr_clf.predict(X_valid)\n",
    "plot_confusion_matrix(y_preds, y_valid, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e740f15c",
   "metadata": {},
   "source": [
    "# Fine-Tuning Transformers\n",
    "\n",
    "Instead of using the AutoModel we will use another one that has the head layer of classifcation and pass to it the number of classes we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be493857",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f09a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_labels = 6\n",
    "\n",
    "model = (AutoModelForSequenceClassification.from_pretrained(model_ckpt, num_labels=num_labels).to(device()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f91d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04667ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    f1 = f1_score(labels, preds, average=\"weighted\")\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\"accuracy\": acc, \"f1\": f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f9e15a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Login successful\n",
      "Your token has been saved to /home/abdelrahman/.huggingface/token\n",
      "\u001b[1m\u001b[31mAuthenticated through git-credential store but this isn't the helper defined on your machine.\n",
      "You might have to re-authenticate when pushing to the Hugging Face Hub. Run the following command in your terminal in case you want to set this credential helper as the default\n",
      "\n",
      "git config --global credential.helper store\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5fcf1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d48214",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2299e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "logging_steps = len(emtions_encoded[\"train\"]) // batch_size\n",
    "\n",
    "model_name = f\"{model_ckpt}-finetuned-emotion\"\n",
    "\n",
    "training_args = TrainingArguments(output_dir=model_name,\n",
    "                                  num_train_epochs=2,\n",
    "                                  learning_rate=2e-5,\n",
    "                                  per_device_train_batch_size=batch_size,\n",
    "                                  per_device_eval_batch_size=batch_size,\n",
    "                                  weight_decay=0.01,\n",
    "                                  evaluation_strategy=\"epoch\",\n",
    "                                  disable_tqdm=False,\n",
    "                                  logging_steps=logging_steps,\n",
    "                                  push_to_hub=True,\n",
    "                                  log_level=\"error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c9cb92",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model=model, args=training_args,\n",
    "                  compute_metrics=compute_metrics,\n",
    "                  train_dataset=emotions_encoded[\"train\"],\n",
    "                  eval_dataset=emotions_encoded[\"validation\"],\n",
    "                  tokenizer=tokenizer)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5c22a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_output = trainer.predict(emotions_encoded[\"validation\"])\n",
    "preds_output.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea7c072",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds = np.argmax(preds_output.predictions, axis=1)\n",
    "plot_confusion_matrix(y_preds, y_valid, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfab1040",
   "metadata": {},
   "source": [
    "# Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7dab824",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.functional import cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24886afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass_with_label(batch):\n",
    "    # Place all input tensors on the same device as the model\n",
    "    inputs = {k:v.to(device) for k,v in batch.items()\n",
    "              if k in tokenizer.model_input_names}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(**inputs)\n",
    "        pred_label = torch.argmax(output.logits, axis=-1)\n",
    "        loss = cross_entropy(output.logits, batch[\"label\"].to(device),\n",
    "                             reduction=\"none\")\n",
    "    # Place outputs on CPU for compatibility with other dataset columns\n",
    "    return {\"loss\": loss.cpu().numpy(),\n",
    "            \"predicted_label\": pred_label.cpu().numpy()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf47fb0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert our dataset back to PyTorch tensors\n",
    "emotions_encoded.set_format(\"torch\",\n",
    "                            columns=[\"input_ids\", \"attention_mask\", \"label\"])\n",
    "# Compute loss values\n",
    "emotions_encoded[\"validation\"] = emotions_encoded[\"validation\"].map(\n",
    "    forward_pass_with_label, batched=True, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed773b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions_encoded.set_format(\"pandas\")\n",
    "cols = [\"text\", \"label\", \"predicted_label\", \"loss\"]\n",
    "df_test = emotions_encoded[\"validation\"][:][cols]\n",
    "df_test[\"label\"] = df_test[\"label\"].apply(label_int2str)\n",
    "df_test[\"predicted_label\"] = (df_test[\"predicted_label\"]\n",
    "                              .apply(label_int2str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8bab80",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.sort_values(\"loss\", ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2606d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.sort_values(\"loss\", ascending=True).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0a0aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.push_to_hub(commit_message=\"Training completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5173477c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93ebee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"A`bdelrahman-Rezk/distilbert-base-uncased-finetuned-emotion\"\n",
    "classifier = pipeline(\"text-classification\", model=model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365a6a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_tweet = \"I saw a movie today and it was really good.\"\n",
    "preds = classifier(custom_tweet, return_all_scores=True)\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e404cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_df = pd.DataFrame(preds[0])\n",
    "plt.bar(labels, 100 * preds_df[\"score\"], color='C0')\n",
    "plt.title(f'\"{custom_tweet}\"')\n",
    "plt.ylabel(\"Class probability (%)\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
