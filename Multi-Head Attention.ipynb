{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b3f38ec2-306c-4c1b-b010-59c5193884c6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
    "from torch import nn\n",
    "import torch\n",
    "from math import sqrt\n",
    "import transformers\n",
    "from typing import List, Tuple\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "45163c5a-0c9e-4b4e-b0f0-f191230b026f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "model_ckpt = \"bert-base-uncased\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "131a8e14-25f1-49a4-bf95-bc06ecaef60e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def scaled_dot_product_attention(query: torch.Tensor, key: torch.Tensor, value: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Applies scaled dot product attention mechanism.\n",
    "\n",
    "    Args:\n",
    "        query (torch.Tensor): Query tensor with shape `(batch_size, query_len, hidden_dim)`.\n",
    "        key (torch.Tensor): Key tensor with shape `(batch_size, key_len, hidden_dim)`.\n",
    "        value (torch.Tensor): Value tensor with shape `(batch_size, value_len, hidden_dim)`.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Attention output tensor with shape `(batch_size, query_len, hidden_dim)`.\n",
    "    \"\"\"\n",
    "    # Get the dimension of the key tensor which the number of laten factors (features) in our case 768\n",
    "    dim_k = query.size(-1)\n",
    "\n",
    "    # Calculate the scaled dot product of the query and key tensors, then scale it\n",
    "    attention_scores = torch.bmm(query, key.transpose(1, 2)) / sqrt(dim_k)\n",
    "\n",
    "    # Apply softmax to the attention scores\n",
    "    weights = torch.nn.functional.softmax(attention_scores, dim=-1)\n",
    "\n",
    "    # Multiply the value tensor with the weights tensor to get the updated version of your inputs, but now it's included the potential encoder + embedding itself,\n",
    "    # which means you have now contextual information in your embedding\n",
    "    attention_output = torch.bmm(weights, value)\n",
    "\n",
    "    return attention_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee30c593-5ade-4a3c-be29-3fb846cd10c1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class AttentionHead(nn.Module):\n",
    "    def __init__(self, embed_dim, head_dim):\n",
    "        \"\"\"\n",
    "        Initializes the AttentionHead module.\n",
    "\n",
    "        Args:\n",
    "            embed_dim: The dimensionality of the input embeddings.\n",
    "            head_dim: The dimensionality of the projected query, key, and value tensors.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # These three projection layers are learned parameters of the model, \n",
    "        # initialized randomly, and trained to fit the data during the training process.\n",
    "       #  Each of these projection layers takes as input a tensor with dimension embed_dim and outputs a tensor with dimension head_dim.\n",
    "        self.q = nn.Linear(embed_dim, head_dim)\n",
    "        self.k = nn.Linear(embed_dim, head_dim)\n",
    "        self.v = nn.Linear(embed_dim, head_dim)\n",
    "        # print('=========================== Query looks like =========================')\n",
    "        # print(self.q)\n",
    "    def forward(self, hidden_state):\n",
    "        \"\"\"\n",
    "        Computes the attention output of the module given the input embeddings.\n",
    "\n",
    "        Args:\n",
    "            hidden_state: A tensor of shape (batch_size, sequence_length, embed_dim) representing the input embeddings.\n",
    "\n",
    "        Returns:\n",
    "            A tensor of shape (batch_size, sequence_length, head_dim) representing the attention output.\n",
    "        \"\"\"\n",
    "        # Compute query, key, and value tensors using the projection layers\n",
    "        # hidden state is our input_embeds of (batch_size, sequence_length, embed_dim)\n",
    "        # But the linear projection will map this hidden_state of size (batch_size, sequence_length, embed_dim) into (batch_size, sequence_length, head_dim)\n",
    "        query = self.q(hidden_state)\n",
    "        key   = self.k(hidden_state)\n",
    "        value = self.v(hidden_state)\n",
    "\n",
    "        # Uncomment to see the size is (batch_size, sequence_length, head_dim) because of the projection\n",
    "        # print(query.size())\n",
    "\n",
    "        # Compute attention using the query, key, and value tensors\n",
    "        # Here is the return was before [batch_size, sequence_length, embed_dim], but because of we project this into head_dim it will be [batch_size, sequence_length, head_dim]\n",
    "        attn_outputs = scaled_dot_product_attention(query, key, value)\n",
    "\n",
    "        # is the same as hidden_state except embed_dim is now the projection size.\n",
    "        # In practice this projection size is multiple of the embed_dim which head_dim = embed_dim / number of heads\n",
    "        # So at the end we will have same embed_dim number but its distributed across different heads\n",
    "        # For example, BERT has 12 attention heads, so the dimension of each head is 768/12 = 64 for each head\n",
    "        print(attn_outputs.size())\n",
    "        return attn_outputs \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8383ff2f-85fb-4150-bcbe-e4c326c814ed",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-head attention layer.\n",
    "\n",
    "    Args:\n",
    "    - config: A configuration object with the following attributes:\n",
    "        * hidden_size: The hidden size of the input tensors.\n",
    "        * num_attention_heads: The number of attention heads to use.\n",
    "    \n",
    "    Attributes:\n",
    "    - heads: A list of `AttentionHead` instances, each representing a single attention head.\n",
    "    - output_linear: A linear layer applied to the concatenated outputs of all the attention heads.\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        # Calculate the dimension of each attention head\n",
    "        embed_dim = config.hidden_size # for beart 768\n",
    "        num_heads = config.num_attention_heads # for bert 12 attention head\n",
    "        head_dim = embed_dim // num_heads # which we project the embed_dim to head_dim so from 768 into 64 \n",
    "\n",
    "        # Create a list of AttentionHead instances, each of these heads are 3 indpendent layers of Q, K, V, each takes embed_dim and return head_dim\n",
    "        self.heads = nn.ModuleList(\n",
    "            [AttentionHead(embed_dim, head_dim) for _ in range(num_heads)]\n",
    "        )\n",
    "        print(len(self.heads))\n",
    "        print('='*50)\n",
    "        print(self.heads)\n",
    "\n",
    "        # Linear layer applied to the concatenated outputs of all the attention heads\n",
    "        self.output_linear = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "\n",
    "    def forward(self, hidden_state):\n",
    "        \"\"\"\n",
    "        Apply multi-head attention to the input tensor.\n",
    "\n",
    "        Args:\n",
    "        - hidden_state: A tensor of shape `(batch_size, seq_len, hidden_size)` representing the input.\n",
    "\n",
    "        Returns:\n",
    "        - A tensor of shape `(batch_size, seq_len, hidden_size)` representing the output of the layer.\n",
    "        \"\"\"\n",
    "        \n",
    "        x = torch.cat([h(hidden_state) for h in self.heads], dim=-1)\n",
    "        print('='*50)\n",
    "        print(x.size())\n",
    "        print(type(x))\n",
    "        print('='*50)\n",
    "        # return back to my original input dims\n",
    "        x = self.output_linear(x)\n",
    "        print(x.size())\n",
    "        print(type(x))\n",
    "        print('='*50)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d2b5ae19-67bb-4341-b563-a413b28685c9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n==================================================\nModuleList(\n  (0): AttentionHead(\n    (q): Linear(in_features=768, out_features=64, bias=True)\n    (k): Linear(in_features=768, out_features=64, bias=True)\n    (v): Linear(in_features=768, out_features=64, bias=True)\n  )\n  (1): AttentionHead(\n    (q): Linear(in_features=768, out_features=64, bias=True)\n    (k): Linear(in_features=768, out_features=64, bias=True)\n    (v): Linear(in_features=768, out_features=64, bias=True)\n  )\n  (2): AttentionHead(\n    (q): Linear(in_features=768, out_features=64, bias=True)\n    (k): Linear(in_features=768, out_features=64, bias=True)\n    (v): Linear(in_features=768, out_features=64, bias=True)\n  )\n  (3): AttentionHead(\n    (q): Linear(in_features=768, out_features=64, bias=True)\n    (k): Linear(in_features=768, out_features=64, bias=True)\n    (v): Linear(in_features=768, out_features=64, bias=True)\n  )\n  (4): AttentionHead(\n    (q): Linear(in_features=768, out_features=64, bias=True)\n    (k): Linear(in_features=768, out_features=64, bias=True)\n    (v): Linear(in_features=768, out_features=64, bias=True)\n  )\n  (5): AttentionHead(\n    (q): Linear(in_features=768, out_features=64, bias=True)\n    (k): Linear(in_features=768, out_features=64, bias=True)\n    (v): Linear(in_features=768, out_features=64, bias=True)\n  )\n  (6): AttentionHead(\n    (q): Linear(in_features=768, out_features=64, bias=True)\n    (k): Linear(in_features=768, out_features=64, bias=True)\n    (v): Linear(in_features=768, out_features=64, bias=True)\n  )\n  (7): AttentionHead(\n    (q): Linear(in_features=768, out_features=64, bias=True)\n    (k): Linear(in_features=768, out_features=64, bias=True)\n    (v): Linear(in_features=768, out_features=64, bias=True)\n  )\n  (8): AttentionHead(\n    (q): Linear(in_features=768, out_features=64, bias=True)\n    (k): Linear(in_features=768, out_features=64, bias=True)\n    (v): Linear(in_features=768, out_features=64, bias=True)\n  )\n  (9): AttentionHead(\n    (q): Linear(in_features=768, out_features=64, bias=True)\n    (k): Linear(in_features=768, out_features=64, bias=True)\n    (v): Linear(in_features=768, out_features=64, bias=True)\n  )\n  (10): AttentionHead(\n    (q): Linear(in_features=768, out_features=64, bias=True)\n    (k): Linear(in_features=768, out_features=64, bias=True)\n    (v): Linear(in_features=768, out_features=64, bias=True)\n  )\n  (11): AttentionHead(\n    (q): Linear(in_features=768, out_features=64, bias=True)\n    (k): Linear(in_features=768, out_features=64, bias=True)\n    (v): Linear(in_features=768, out_features=64, bias=True)\n  )\n)\n"
     ]
    }
   ],
   "source": [
    "model_config = AutoConfig.from_pretrained(model_ckpt)\n",
    "multihead_attn = MultiHeadAttention(model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3fcf19b7-bbec-4526-88b3-a08e84d623ba",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "text = [\"time flies like an arrow and tokenizer\"]\n",
    "# Call the tokenizer related to this model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
    "# send the tokenizer and text to get the input_ids and attention_mask from the returned inputs\n",
    "tokenized_texts = tokenized_texts = tokenizer.batch_encode_plus(\n",
    "        text,\n",
    "        return_tensors=\"pt\",  # Return PyTorch tensors\n",
    "        add_special_tokens=False  # Add special tokens if required\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b4aba57-377c-4316-847e-06395d134763",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']\n- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = AutoModel.from_pretrained(model_ckpt)\n",
    "\n",
    "input_ids = tokenized_texts[\"input_ids\"]\n",
    "attention_masks = tokenized_texts[\"attention_mask\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7780852e-4225-4462-ac67-383a9df32166",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 8, 64])\ntorch.Size([1, 8, 64])\ntorch.Size([1, 8, 64])\ntorch.Size([1, 8, 64])\ntorch.Size([1, 8, 64])\ntorch.Size([1, 8, 64])\ntorch.Size([1, 8, 64])\ntorch.Size([1, 8, 64])\ntorch.Size([1, 8, 64])\ntorch.Size([1, 8, 64])\ntorch.Size([1, 8, 64])\ntorch.Size([1, 8, 64])\n==================================================\ntorch.Size([1, 8, 768])\n<class 'torch.Tensor'>\n==================================================\ntorch.Size([1, 8, 768])\n<class 'torch.Tensor'>\n==================================================\nOut[23]: torch.Size([1, 8, 768])"
     ]
    }
   ],
   "source": [
    "\n",
    "with torch.no_grad():\n",
    "        inputs_embeds = model(input_ids)[0]\n",
    "\n",
    "attn_output = multihead_attn(inputs_embeds)\n",
    "attn_output.size()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ba162099-de5b-46a4-b3bd-b4830b3c4c5c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "Multi-Head Attention",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
